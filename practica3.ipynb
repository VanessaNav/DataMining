{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3 - Multiclasificadores y selección de variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minería de Datos 2017/2018 - Jacinto Arias y José A. Gámez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividiremos esta práctica en dos partes:\n",
    "\n",
    "**Multiclasificadores:** estudiaremos la API de scikit para los modelos de tipo ensemble y veremos como entrenar, seleccionar y utilizar estos modelos.\n",
    "\n",
    "**Selección de variables:** veremos los distintos algoritmos de selección de variables disponibles en scikit y como aplicarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always load all scipy stack packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, integrate\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code configures matplotlib for proper rendering\n",
    "%matplotlib inline\n",
    "mpl.rcParams[\"figure.figsize\"] = \"8, 4\"\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=6342\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "Esta vez vamos a utilizar los datos obtenidos directamente del benchmark de scikit learn [Docs](http://scikit-learn.org/stable/datasets/index.html#optical-recognition-of-handwritten-digits-data-set). Concretamente utilizaremos la versión completa del dataset Wisconsin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "wisconsin = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos datasets vienen preparados directamente para la clasificación y separan en un diccionario los distintos elementos que nos puedan interesar. Si quisieramos recuperar los datos en un data.frame tendríamos que combinar los elementos correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wisconsin.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data = np.column_stack((wisconsin.data, wisconsin.target)), \n",
    "    columns = np.append(wisconsin.feature_names, \"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>14.87</td>\n",
       "      <td>20.21</td>\n",
       "      <td>96.12</td>\n",
       "      <td>680.9</td>\n",
       "      <td>0.09587</td>\n",
       "      <td>0.08345</td>\n",
       "      <td>0.06824</td>\n",
       "      <td>0.04951</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.05748</td>\n",
       "      <td>...</td>\n",
       "      <td>28.48</td>\n",
       "      <td>103.90</td>\n",
       "      <td>783.6</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>0.13880</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>0.06599</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>11.31</td>\n",
       "      <td>19.04</td>\n",
       "      <td>71.80</td>\n",
       "      <td>394.1</td>\n",
       "      <td>0.08139</td>\n",
       "      <td>0.04701</td>\n",
       "      <td>0.03709</td>\n",
       "      <td>0.02230</td>\n",
       "      <td>0.1516</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.84</td>\n",
       "      <td>78.00</td>\n",
       "      <td>466.7</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>0.09148</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.06961</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.06641</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>11.08</td>\n",
       "      <td>14.71</td>\n",
       "      <td>70.21</td>\n",
       "      <td>372.7</td>\n",
       "      <td>0.10060</td>\n",
       "      <td>0.05743</td>\n",
       "      <td>0.02363</td>\n",
       "      <td>0.02583</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>...</td>\n",
       "      <td>16.82</td>\n",
       "      <td>72.01</td>\n",
       "      <td>396.5</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>0.08240</td>\n",
       "      <td>0.03938</td>\n",
       "      <td>0.04306</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.07313</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>15.04</td>\n",
       "      <td>16.74</td>\n",
       "      <td>98.73</td>\n",
       "      <td>689.4</td>\n",
       "      <td>0.09883</td>\n",
       "      <td>0.13640</td>\n",
       "      <td>0.07721</td>\n",
       "      <td>0.06142</td>\n",
       "      <td>0.1668</td>\n",
       "      <td>0.06869</td>\n",
       "      <td>...</td>\n",
       "      <td>20.43</td>\n",
       "      <td>109.70</td>\n",
       "      <td>856.9</td>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.21760</td>\n",
       "      <td>0.18560</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.2177</td>\n",
       "      <td>0.08549</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.86</td>\n",
       "      <td>104.30</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.09495</td>\n",
       "      <td>0.08501</td>\n",
       "      <td>0.05500</td>\n",
       "      <td>0.04528</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>0.05875</td>\n",
       "      <td>...</td>\n",
       "      <td>19.58</td>\n",
       "      <td>115.90</td>\n",
       "      <td>947.9</td>\n",
       "      <td>0.1206</td>\n",
       "      <td>0.17220</td>\n",
       "      <td>0.23100</td>\n",
       "      <td>0.11290</td>\n",
       "      <td>0.2778</td>\n",
       "      <td>0.07012</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "495        14.87         20.21           96.12      680.9          0.09587   \n",
       "67         11.31         19.04           71.80      394.1          0.08139   \n",
       "173        11.08         14.71           70.21      372.7          0.10060   \n",
       "500        15.04         16.74           98.73      689.4          0.09883   \n",
       "406        16.14         14.86          104.30      800.0          0.09495   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "495           0.08345         0.06824              0.04951         0.1487   \n",
       "67            0.04701         0.03709              0.02230         0.1516   \n",
       "173           0.05743         0.02363              0.02583         0.1566   \n",
       "500           0.13640         0.07721              0.06142         0.1668   \n",
       "406           0.08501         0.05500              0.04528         0.1735   \n",
       "\n",
       "     mean fractal dimension  ...    worst texture  worst perimeter  \\\n",
       "495                 0.05748  ...            28.48           103.90   \n",
       "67                  0.05667  ...            23.84            78.00   \n",
       "173                 0.06669  ...            16.82            72.01   \n",
       "500                 0.06869  ...            20.43           109.70   \n",
       "406                 0.05875  ...            19.58           115.90   \n",
       "\n",
       "     worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "495       783.6            0.1216            0.13880          0.17000   \n",
       "67        466.7            0.1290            0.09148          0.14440   \n",
       "173       396.5            0.1216            0.08240          0.03938   \n",
       "500       856.9            0.1135            0.21760          0.18560   \n",
       "406       947.9            0.1206            0.17220          0.23100   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  label  \n",
       "495               0.10170          0.2369                  0.06599    1.0  \n",
       "67                0.06961          0.2400                  0.06641    1.0  \n",
       "173               0.04306          0.1902                  0.07313    1.0  \n",
       "500               0.10180          0.2177                  0.08549    1.0  \n",
       "406               0.11290          0.2778                  0.07012    1.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podéis observar, la variable `label` no está codificada como categories, sino como una variable contínua. Esta es la forma preferida de representación en la mayoría de librerías de Machine Learning, ya que optimiza el indexado y otras operaciones. No obstante, hemos de tener cuidado para asegurarnos que todos los algoritmos y técnicas que utilicemos entienden que se trata de un problema de clasificación y no de regresion, ya que en este caso no hay forma de distinguirlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede que en algunos casos queramos disponer de la variable clase como una categoría, especialmente para realizar análisis exploratorio y generar gráficas, para ello necesitamos utilizar las herramientas de pandas para representar datos categóricos.\n",
    "\n",
    "**Nota:** Si buscáis sobre este problema lo más común es encontrar soluciones al problema contrario, como codificar mis variables categóricas como numéricas o en general hacer *scrapping* a partir de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       "287  benign\n",
       "55   benign\n",
       "455  benign"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_categorical = pd.Series(\n",
    "     pd.Categorical([wisconsin.target_names[x] for x in wisconsin.target], \n",
    "     categories=wisconsin.target_names)\n",
    ")\n",
    "\n",
    "dfLabel = pd.DataFrame(target_categorical, columns=[\"label\"])\n",
    "dfLabel.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independientemente, realizaremos el resto de la práctica utilizando las versiones numéricas de los datos para demostrar su versatilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = wisconsin.data\n",
    "label = wisconsin.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles en scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería implementa una gran colección de las técnicas más populares de modelos ensemble. Os recomiendo la lectura del apartado correspondiente de la documentación ya que esclarece bastante bien los diversos detalles de implementación de cada algoritmo. Podéis hacerlo en el siguiente [Enlace](http://scikit-learn.org/stable/modules/ensemble.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree como referencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder estudiar la efectividad de los algoritmos anteriores evaluaremos un árbol de decisión sin poda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el arbol de decision\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "scores_dt = cross_val_score(estimator=dt, X=attributes, y=label, scoring=\"accuracy\", cv=3)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_dt.mean(), scores_dt.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "La estrategia básica tras el algoritmo de **bagging** es la agregación de distintos clasificadores que han sido aprendidos a partir de muestras obtenidas tras un proceso de bootstraping (muestreo con remplazo). Aquí estudiaremos cómo aprender un modelo de este tipo utilizando scikit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sklearn.ensemble.BaggingClassifier(base_estimator=None, (El clasificador de base que usará el ensemble) \n",
    "                                   n_estimators=10, (Numero de modelos que aprenderemos) \n",
    "                                   max_samples=1.0, (proporción de la muestra a utilizar)\n",
    "                                   max_features=1.0, (proporcion de características a  utilizar)\n",
    "                                   bootstrap=True, (si se utilizará muestreo por remplazo o no)\n",
    "                                   n_jobs=1, (para utilizar paralelismo, debe soportarlo nuestro entorno)\n",
    "                                   random_state=None (la semilla)\n",
    "                                  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación aprenderemos un ensemble de árboles de decisión mediante bagging, para ello, hemos de suministrarle el modelo a utilizar, en nuestro caso: `tree.DecisionTreeClassifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagg = BaggingClassifier(tree.DecisionTreeClassifier(), \n",
    "                          n_estimators = 30, \n",
    "                          random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "scores_bagg = cross_val_score(bagg, attributes, label, cv=3, scoring=\"accuracy\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_bagg.mean(), scores_bagg.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Una alternativa muy popular al bagging es el clasificador **random forest**. En este caso también se utiliza muestreo con remplazo, pero adicionalmente integra otras técnicas de aleatorización en el aprendizaje de los árboles de decisión para ampliar la generalización del ensemble. Concretamente utiliza una muestra aleatoria de los atributos a la hora de seleccionar cada punto óptimo de corte.\n",
    "\n",
    "Un clasificador random forest siempre utiliza árboles de decisión como submodelos y por ello no requiere un clasificador base para su definición. Por esa misma razón los hyperparámetros de este clasificador incluyen tanto los correspondientes al ensemble como al árbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sklearn.ensemble.RandomForestClassifier(n_estimators=10, (numero de modelos que aprenderemos)\n",
    "                                        criterion='gini', (parametro del arbol de decision)\n",
    "                                        max_depth=None, (parametro del arbol de decision)\n",
    "                                        min_samples_split=2, (parametro del arbol de decision)\n",
    "                                        min_samples_leaf=1, (parametro del arbol de decision)\n",
    "                                        min_impurity_split=1e-07, (parametro del arbol de decision)\n",
    "                                        max_features='auto', (número de atributos que se usan en cada caso:\n",
    "                                                              puede ser un entero para número exacto, un float\n",
    "                                                              para proporcion o 'auto', 'sqrt', 'log2' o 'None')\n",
    "                                        bootstrap=True, (si se utilizará muestreo por remplazo)\n",
    "                                        n_jobs=1, (para utilizar paralelismo, debe soportarlo nuestro entorno)\n",
    "                                        random_state=None, (semilla)\n",
    "                                        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=30, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "scores_rf = cross_val_score(rf, attributes, label, cv=3, scoring=\"accuracy\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_rf.mean(), scores_rf.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "La estrategia de boosting es completamente diferente a la anterior, ya que en lugar de reducir la varianza al aprender múltiples clasificadores independientes, realizaremos un proceso iterativo en el que intensificaremos el aprendizaje sobre aquellas instancias más complicadas de aprender para nuestro modelo. \n",
    "El algoritmo más conocido se llama `AdaBoost` y está disponible en scikit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sklearn.ensemble.AdaBoostClassifier(base_estimator=None, (El clasificador de base que usará el ensemble) \n",
    "                                    n_estimators=50, (numero de modelos que aprenderemos)\n",
    "                                    learning_rate=1.0, (la contribución del modelo anterior al siguiente)\n",
    "                                    random_state=None (semilla)\n",
    "                                    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#boost = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(), n_estimators=30)\n",
    "boost = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=3), n_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "scores_boost = cross_val_score(boost, attributes, label, cv=3, scoring=\"accuracy\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_boost.mean(), scores_boost.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Trees\n",
    "Es una generalización del algoritmo de boosting que optimiza funciones de coste en el dominio concreto de los árboles de decisión. Actualmente es una de las técnicas más exitosas para resolver problemas de clasificación complejos *off of the shelf*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sklearn.ensemble.GradientBoostingClassifier(n_estimators=100, (numero de modelos que aprenderemos)\n",
    "                                            learning_rate=0.1, (la contribución del modelo anterior)\n",
    "                                            subsample=1.0, (aprender mediante submuestreo)\n",
    "                                            max_features=None, (submuestrear atributos como en RF)\n",
    "                                            min_samples_split=2, (parametro del arbol de decision)\n",
    "                                            min_samples_leaf=1, (parametro del arbol de decision)\n",
    "                                            max_depth=3, (parametro del arbol de decision)\n",
    "                                            min_impurity_split=1e-07, (parametro del arbol de decision)\n",
    "                                            random_state=None, (semilla)\n",
    "                                            )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gboost = GradientBoostingClassifier(n_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "scores_gboost = cross_val_score(gboost, attributes, label, cv=5, scoring=\"accuracy\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_gboost.mean(), scores_gboost.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación\n",
    "Cualquier técnica de ensemble debería ser capaz de mejorar el rendimiento del clasificador base, aunque el rendimiento entre ellas puede depender enormemente del **problema** y de los **parámetros** con los que se hayan configurado las técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to make a HTML table!\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def printTable(list):\n",
    "    table = \"\"\"<table>%s</table>\"\"\"\n",
    "    row = \"\"\"<tr>%s</tr>\"\"\"\n",
    "    cell = \"\"\"<td>%s</td>\"\"\"\n",
    "    report =  table % ''.join([row % (cell % x[0] + cell % x[1]) for x in results])\n",
    "    display(HTML(report))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Decision Tree</td><td>0.906850459482</td></tr><tr><td>Bagging</td><td>0.934939199851</td></tr><tr><td>Random Forest</td><td>0.954311705189</td></tr><tr><td>Boosting</td><td>0.970138308735</td></tr><tr><td>Gradient Boosting</td><td>0.95444401693</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = ((\"Decision Tree\", scores_dt.mean()), \n",
    "           (\"Bagging\", scores_bagg.mean()), \n",
    "           (\"Random Forest\", scores_rf.mean()),\n",
    "           (\"Boosting\", scores_boost.mean()),\n",
    "           (\"Gradient Boosting\", scores_gboost.mean()))\n",
    "\n",
    "printTable(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategias de reducción de varianza vs estrategias de reducción de sesgo (variance and bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque la estrategia común detrás de un ensemble sea la de generar un consenso entre varios modelos (mediante agregación o voto por ejemplo), la forma en la que se genera el conjunto de modelos está dirigida por motivaciones muy diferentes para algoritmos como bagging o boosting. Cuando utilizamos una u otra técnica es fundamental conocer el fundamento interno de la técnica de ensembling para seleccionar y aprender correctamente los modelos del ensemble.\n",
    "\n",
    "En el caso de **bagging** o **random forest** queremos utilizar una función de aprendizaje (C4.5, CART...) y obtener modelos \"diversos\" a partir de los mismos datos para reducir el error obtenido mediante **varianza**. Formalmente hablamos de clasificadores que individualmente tengan poder de generalización pero que al mismo tiempo estén lo menos correlados entre ellos. Es por ello que se utilizan diversas técnicas de submuestreo y aleatorización de los modelos. \n",
    "\n",
    "En el caso de los algoritmos de **boosting** la estrategia es opuesta, dado un problema complejo queremos reducir el sesgo en nuestros modelos. Por ello en lugar de aleatorización guíamos la función de aprendizaje para que incida en aquellos ejemplos que sean más dificiles de generalizar. La técnica general es utilizar sobremuestreo o pesos para que sobreajustar el clasificador en aquellos casos conflictivos, de manera más formal o sofisticada se puede establecer un problema de optimización de funciones de coste.\n",
    "\n",
    "Para enteder este problema hay que saber responder a las siguientes preguntas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¿Cuáles son los mejores clasificadores para ejecutar algoritmos de bagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'V2VhayBsZWFybmVycywgbyBjbGFzaWZpY2Fkb3JlcyBjb24gdW4gcG9kZXIgZGUgZ2VuZXJhbGl6YWNpb24geSBwYXJhbWV0cm9zIG5vIG11eSBjb21wbGVqb3Mgbmkgc29icmVhanVzdGFkb3MuIEVzdG8gb3RvcmdhIGFsIGFsZ29yaXRtbyBkZSBib29zdGluZyBlc3BhY2lvIHBhcmEgb3B0aW1pemFyIGRpY2hvcyBwYXJhbWV0cm9zIHkgdGVvcmljYW1lbnRlIHRyYW5zZm9ybWFyIGFsIGNsYXNpZmljYWRvciBiYXNlIGVuIHVuIHN0cm9uZyBsZWFybmVyLiBQb3IgZWplbXBsbywgYXJib2xlcyBkZSBkZWNpc2lvbiBtdXkgcG9jbyBwcm9mdW5kb3MgKHNoYWxsb3cgdHJlZXMp'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64.b64encode(b'Weak learners, o clasificadores con un poder de generalizacion y parametros no muy complejos ni sobreajustados. Esto otorga al algoritmo de boosting espacio para optimizar dichos parametros y teoricamente transformar al clasificador base en un strong learner. Por ejemplo, arboles de decision muy poco profundos (shallow trees)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Strong learners, clasificadores con un gran poder predictivo en sus parametros dando pie a utilizar modelos que presenten mucha varianza, ya que esta se reduce, como es el caso de arboles de decision sin podar. '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run me to know the answer (b64 encoded quizes FTW!)\n",
    "base64.b64decode(b'U3Ryb25nIGxlYXJuZXJzLCBjbGFzaWZpY2Fkb3JlcyBjb24gdW4gZ3JhbiBwb2RlciBwcmVkaWN0aXZvIGVuIHN1cyBwYXJhbWV0cm9zIGRhbmRvIHBpZSBhIHV0aWxpemFyIG1vZGVsb3MgcXVlIHByZXNlbnRlbiBtdWNoYSB2YXJpYW56YSwgeWEgcXVlIGVzdGEgc2UgcmVkdWNlLCBjb21vIGVzIGVsIGNhc28gZGUgYXJib2xlcyBkZSBkZWNpc2lvbiBzaW4gcG9kYXIuIA==')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¿Cuáles son los mejores clasificadores para ejecutar algoritmos de boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Weak learners, o clasificadores con un poder de generalizacion y parametros no muy complejos ni sobreajustados. Esto otorga al algoritmo de boosting espacio para optimizar dichos parametros y teoricamente transformar al clasificador base en un strong learner. Por ejemplo, arboles de decision muy poco profundos (shallow trees)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run me to know the answer (b64 encoded quizes FTW!)\n",
    "base64.b64decode(b'V2VhayBsZWFybmVycywgbyBjbGFzaWZpY2Fkb3JlcyBjb24gdW4gcG9kZXIgZGUgZ2VuZXJhbGl6YWNpb24geSBwYXJhbWV0cm9zIG5vIG11eSBjb21wbGVqb3Mgbmkgc29icmVhanVzdGFkb3MuIEVzdG8gb3RvcmdhIGFsIGFsZ29yaXRtbyBkZSBib29zdGluZyBlc3BhY2lvIHBhcmEgb3B0aW1pemFyIGRpY2hvcyBwYXJhbWV0cm9zIHkgdGVvcmljYW1lbnRlIHRyYW5zZm9ybWFyIGFsIGNsYXNpZmljYWRvciBiYXNlIGVuIHVuIHN0cm9uZyBsZWFybmVyLiBQb3IgZWplbXBsbywgYXJib2xlcyBkZSBkZWNpc2lvbiBtdXkgcG9jbyBwcm9mdW5kb3MgKHNoYWxsb3cgdHJlZXMp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si observamos la configuración de los experimentos anteriores podemos ver que en el caso de random forest, el árbol de decisión no está podado por defecto mientras que en gboost los árboles tienen una profundidad máxima de 3. Este el motivo por el que el algoritmo básico de boosting no esta funcionando correctamente. Probad a modificar los parámetros de los meta-algoritmos y del algoritmo base para confirmar el comportamiento anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergencia respecto al número de modelos\n",
    "\n",
    "Una duda común en el uso de técnicas de *ensembling* surge a la hora de fijar el número de modelos a aprender. Es importante conocer como se comportan los clasificadores conforme el número de modelos aumenta. Ambos clasificadores convergen de una forma diferente.\n",
    "\n",
    "En el caso de bagging, para cada problema y configuración, el clasificador se estabiliza conforme se agregan más modelos al ensemble hasta llegar a un punto de convergencia en el cual la varianza se ha reducido \"todo lo posible\" y no mejorarán los resultados. La moraleja es que un número elevado de modelos mejorará los resultados, pero que siempre hay un punto en el que hay \"suficientes\", pasar de este punto no empeorará teóricamente nuestro modelo pero sí aumentará enormente el tiempo de aprendizaje y clasificación.\n",
    "\n",
    "En el caso de boosting el problema es más complejo, y las connotaciones teóricas tienen mayor implicación. Al tratarse de un problema de optimización el algoritmo converge de una forma diferente ya que sobreajusta a los datos. Es por ello que es necesario elegir mejor el número de modelos, no solo para limitar el número de recursos que necesitemos sino también para no sobreajustar demasiado los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complejidad temporal (y espacial) en ensembles\n",
    "\n",
    "Como hemos anticipado, los meta-clasificadores basados en ensembles son muy potentes. Esta mejora en la clasificación la ganamos a cambio de recursos computacionales, ya que multiplicamos directamente el número de recursos necesarios en comparación con un algoritmo de clasificación básica.\n",
    "\n",
    "La complejidad viene acotada por el número de modelos, por lo que la cantidad de recursos de los que dispongamos tendrá que ser un dactor determinante a la hora de fijar este valor.\n",
    "\n",
    "**Paralelismo:** Cada día es más común encontrar plataformas de cómputo paralelo, especialmente desde la explosión del Big Data. En este caso, los algoritmos de boosting pueden ser fácilmente adaptados para ser optimizados en paralelo, mientras que los algoritmos de boosting, al ser procesos iterativos, no se benefician de las mismas ventajas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree\n",
      "10.5 ms ± 1.87 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n",
      "\n",
      "Random Forest\n",
      "272 ms ± 23.1 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n",
      "\n",
      "Gradient Boosting\n",
      "246 ms ± 21.1 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "# TRAINING TIME\n",
    "\n",
    "print(\"\\nDecision Tree\")\n",
    "model = tree.DecisionTreeClassifier()\n",
    "%timeit -n 5 model.fit(attributes, label)\n",
    "\n",
    "print(\"\\nRandom Forest\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=1234)\n",
    "%timeit -n 5 model.fit(attributes, label)\n",
    "\n",
    "print(\"\\nGradient Boosting\")\n",
    "model = GradientBoostingClassifier(n_estimators=100)\n",
    "%timeit -n 5 model.fit(attributes, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección de variables en scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como hemos visto en clase, existen distintas estrategias a la hora de reducir la dimensionalidad de un problema. Aunque scikit incorpora una serie de algoritmos y métricas de selección de variables [Docs](http://scikit-learn.org/stable/modules/feature_selection.html), su catálogo es limitado con respecto al estado del arte actual en la literatura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos filter univariados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de variables con baja varianza\n",
    "\n",
    "Resulta obtvio que si una variable presenta valores de varianza muy reducidos (por ejemplo una constante) no será un buen atributo predictor. Aunque este método pertenece más a la familia de métodos de preprocesamiento y eliminación de ruido sklearn lo clasifica como un método de preprocesado y así lo incluye en su API.\n",
    "\n",
    "```\n",
    "VarianceThreshold(\n",
    "    threshold = 0.0 # Valor mínimo de la varianza para considerarse ruidoso\n",
    "```\n",
    "\n",
    "Como veréis a continuación, los métodos de selección de variables presentan la misma API que cualquier algoritmo de transformación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 31)\n",
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# We add a noisy constant column\n",
    "noise = np.ones(attributes.shape[0])\n",
    "noisy_atts = np.column_stack((attributes, noise))\n",
    "print( noisy_atts.shape )\n",
    "\n",
    "# We add a noisy constant column\n",
    "sel = VarianceThreshold()\n",
    "sel.fit(noisy_atts)\n",
    "print( sel.transform(noisy_atts).shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección por rankings\n",
    "\n",
    "Scikit implementa el algoritmo filter más sencillo de todos, la selección por rankings. Para ello ordena todas los atributos dada una métrica y selecciona los `k`mejores para el clasificador. Distintas versiones del algoritmo nos permiten especificar `k`como una constante o una proporción.\n",
    "\n",
    "Para ejecutar este algoritmo necesitaremos no solo la estrategia de ranking sino la implementación de la métrica correspondiente. Scikit provee algunas de las más comunes como la información mútua.\n",
    "\n",
    "```\n",
    "sklearn.feature_selection.mutual_info_classif   # Información mútua\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.47518525067526252, 'worst perimeter'),\n",
       " (0.4630635297199055, 'worst area'),\n",
       " (0.45515685867696409, 'worst radius'),\n",
       " (0.44046272810851894, 'mean concave points'),\n",
       " (0.43785096902449139, 'worst concave points'),\n",
       " (0.40429591576647761, 'mean perimeter'),\n",
       " (0.37435430813985771, 'mean concavity'),\n",
       " (0.36957310392853637, 'mean radius'),\n",
       " (0.35859977750084582, 'mean area'),\n",
       " (0.33913602338779492, 'area error'),\n",
       " (0.31686394124406769, 'worst concavity'),\n",
       " (0.27361385712564101, 'perimeter error'),\n",
       " (0.24870957033219399, 'radius error'),\n",
       " (0.22752331946337234, 'worst compactness'),\n",
       " (0.21223646713681044, 'mean compactness'),\n",
       " (0.12794646349638605, 'concave points error'),\n",
       " (0.11960002324391539, 'concavity error'),\n",
       " (0.11899047482845848, 'worst texture'),\n",
       " (0.096465466606317385, 'worst smoothness'),\n",
       " (0.095982484270855428, 'mean texture'),\n",
       " (0.092908406557946632, 'worst symmetry'),\n",
       " (0.076624709670718216, 'compactness error'),\n",
       " (0.075970717512003905, 'mean smoothness'),\n",
       " (0.071999722574530134, 'mean symmetry'),\n",
       " (0.068450675292411667, 'worst fractal dimension'),\n",
       " (0.038272914449894113, 'fractal dimension error'),\n",
       " (0.014016435095338009, 'smoothness error'),\n",
       " (0.013404641699180653, 'symmetry error'),\n",
       " (0.0095863137209002414, 'mean fractal dimension'),\n",
       " (0.0, 'texture error')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# We can compute the scores and ranking directly:\n",
    "\n",
    "scores = list(mutual_info_classif(attributes, label, random_state=seed))\n",
    "names = list(wisconsin.feature_names)\n",
    "ranks = sorted( list(zip(scores, names)), reverse=True )\n",
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar esta métrica en nuestro análisis exploratorio si lo deseamos, no obstante, como algoritmo de selección de variables es fundamental que integremos la selección dentro de nuestro proceso de selección, por lo que debemos utilizar un método de transformación:\n",
    "\n",
    "```\n",
    "sklearn.feature_selection.SelectKBest(\n",
    "    score_func  # Métrica a optimizar\n",
    "    k=10 # Número de atributos a seleccionar\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean concave points' 'worst radius' 'worst perimeter' 'worst area'\n",
      " 'worst concave points']\n",
      "(569, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "# We configure and train the algorithm\n",
    "fss = SelectKBest(mutual_info_classif, k=5).fit(attributes, label)\n",
    "\n",
    "# We can obtain the mask of the selected attributes\n",
    "print(wisconsin.feature_names[fss.get_support()])\n",
    "\n",
    "# We can transform the dataset to project only the selected attributes\n",
    "print(fss.transform(attributes).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de importancia de las variables\n",
    "\n",
    "Algunos clasificadores pueden asignar una métrica de importancia a sus variables a partir de los parámetros obtenidos durante el algoritmo de aprendizaje. Por ejemplo, en una regresión podemos fijarnos en los pesos de los coeficientes de las variables. Una de las métricas más utilizadas es la métrica de importancia obtenida en un ensemble de árboles, donde la importancia se calcula a partir de la influencia media que tiene cada variable en el proceso de construcción de los árboles (en función de cuántas veces es seleccionada y a cuantas instancias implica).\n",
    "\n",
    "Esta métrica puede utilizarse para establecer un ranking entre las variables para seleccionar un subconjunto óptimo. Este proceso puede realizarse de manera independiente al aprendizaje el clasificador, por lo que podemos combinar modelos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.13280182304084834, 'worst area'),\n",
       " (0.12522441553801289, 'worst concave points'),\n",
       " (0.11957281896310926, 'mean concave points'),\n",
       " (0.11305112291391831, 'worst radius'),\n",
       " (0.10725106470402063, 'worst perimeter'),\n",
       " (0.065995111613632348, 'mean concavity'),\n",
       " (0.055120669821861999, 'mean perimeter'),\n",
       " (0.046758683491729985, 'worst concavity'),\n",
       " (0.033865123475926698, 'area error'),\n",
       " (0.033228454027190352, 'mean area'),\n",
       " (0.02712100085555047, 'mean radius'),\n",
       " (0.023688538525107553, 'worst texture'),\n",
       " (0.015157495706731283, 'mean texture'),\n",
       " (0.012474648157482209, 'worst smoothness'),\n",
       " (0.011416966382167066, 'mean compactness'),\n",
       " (0.010083105240270118, 'radius error'),\n",
       " (0.0082233080270945544, 'worst symmetry'),\n",
       " (0.0072876605422124438, 'worst compactness'),\n",
       " (0.0061961130203895976, 'concavity error'),\n",
       " (0.0054616896259530598, 'worst fractal dimension'),\n",
       " (0.0054390259088372191, 'mean smoothness'),\n",
       " (0.0049092939420526775, 'concave points error'),\n",
       " (0.0048022705043699009, 'perimeter error'),\n",
       " (0.0042631965193962165, 'symmetry error'),\n",
       " (0.0041089151429092169, 'texture error'),\n",
       " (0.0039510261492700363, 'compactness error'),\n",
       " (0.0036059780291123524, 'fractal dimension error'),\n",
       " (0.0032096174351657573, 'mean fractal dimension'),\n",
       " (0.0031845809974325899, 'smoothness error'),\n",
       " (0.0025462816982449198, 'mean symmetry')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "classifier = model.fit(attributes, label)\n",
    "\n",
    "# We can obtain the importances for any model capable of computing them\n",
    "scores = classifier.feature_importances_\n",
    "names = list(wisconsin.feature_names)\n",
    "ranks = sorted( list(zip(scores, names)), reverse=True )\n",
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit nos permite introducir esta información durante la fase de preprocesado mediante la funcion `SelectFromModel`. En este caso es importante que suministremos al algoritmo con un modelo aprendido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "fss = SelectFromModel(estimator = model, threshold = \"mean\").fit(attributes, label)\n",
    "fss.transform(attributes).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda recursiva\n",
    "\n",
    "La alternativa más popular, aunque más costosa en recursos computacionales son los métodos wrapper de búsqueda. En este caso utilizaremos el rendimiento del propio clasificador en un proceso de validación cruzada a modo de score para guiar una búsqueda **greedy** o exhaustiva por el espacio de búsqueda.\n",
    "\n",
    "Generalmente se utilizan dos alternativas:\n",
    "\n",
    "* **Busqueda Backward**: Se comienza por un clasificador que utiliza todos los atributos y se van eliminando las peores variables hasta que el resultado no mejora.\n",
    "\n",
    "* **Búsqueda Forward**: Se comienza con una sola variable y se van añadiendo las más prometedoras hasta que el resultado no mejora.\n",
    "\n",
    "En ocasiones se guía este proceso de búsqueda utilizando un orden preestablecido entre las variables, por ejemplo, mediante la aplicación de un algoritmo de ranking.\n",
    "\n",
    "Lamentablemente, scikit learn no implementa directamente estos algoritmos en su API de selección de variables. No obstante, su definición es muy sencilla ya que al tratarse de métodos púramente wrappers no tienen cabida dentro de un pipeline y podrían ejecutarse desde fuera del bucle de validación cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos filter multivariados\n",
    "Desafortunadamente scikit learn no implementa ningún método de filtering multivariado, al contrario que otras librerías como **weka**. No obstante, no es dificil implementar estos métodos haciendo uso de los estadísticos que sí podemos computar. Como la aproximación de la información mútua (Battiti) o la correlación entre las variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos de la práctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta práctica tendremos dos objetivos principales:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Uso y ajuste de métodos basados en ensembles\n",
    "\n",
    "* **A.1) Implementación básica de un ensemble de manera manual:**\n",
    "    ```\n",
    "        Pseudocodigo básico:\n",
    "        Generar una lista de datos muestreados (con/sin remplazo, con/sin muestreo de atributos)\n",
    "        Aprender un clasificador para cada muestra\n",
    "        Obtener predicciones para cada modelo\n",
    "        Obtener la moda de dichas predicciones\n",
    "    ```\n",
    "Implementar vuestro propio ensemble, con la API más similar posible a la de scikit learn (aquí seremos flexibles). Se valorará probar distintas estrategias de randomización y explicar el rendimiento de cada una.\n",
    "\n",
    "--> funcion pandas sample: usarla en un bucle. Aprender un modelo en cada iteracion y guardarlos en un vector de modelos (Bagging simple) --> mejorarlo para obtener puntos extra (usar cv).\n",
    "    \n",
    "*  **A.2) Uso de Bagging, Boosting, Random Forest y Gradient Boosting**: Aprender y ajustar los parámetros de los distintos modelos para maximizar la clasificación. Comparar estos resultados con el ensemble implementado manualmente.\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Selección de variables\n",
    "\n",
    "Al igual que en el caso anterior, se busca que el alumno intente **mejorar** los resultados obenidos utilizando clasificadores base o ensembles al aplicar técnicas de selección de variables.\n",
    "\n",
    "* **B.1 Utilizar un método filter basado en rankings y la importancia de las variables para evaluar distintos subconjuntos** --> funciones ranking y importance usando un grid serach\n",
    "* **B.2 Implementar al menos un algoritmo de búsqueda recursiva wrapper** --> bucle for, fuera de la cv: una iteracion de cv, hacer wrapper, otra iteracion, otro wrapper...\n",
    "* **B.3 (Opcional, recomendable) Implementar un algoritmo greedy basado en métricas filter (información mútua). si no se puede integrar el algoritmo en el proceso de validación (cross validation) se puede probar mediante evaluación holdout** --> por ejemplo un batiti\n",
    "\n",
    "Se deberán implementar los experimentos para poderlos ejecutar para distintos conjuntos de datos. Se reportarán como mínimo los resultados para wisconsin y pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
