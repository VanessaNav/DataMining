{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vanessa Navarro Coronado e Iván Sánchez Castellanos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2 - Clasificación supervisada en scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta práctica vamos a estudiar de nuevo los datasets Pima y Wisconsin. Probaremos los clasificadores de Árbol de Decisión y KNN, y analizaremos cuál es mejor en cada caso. Además, trataremos de descubrir las configuraciones óptimas de dichos algoritmos, y realizaremos un breve estudio sobre ellos.\n",
    "\n",
    "También realizaremos la parte opcional, en la aplicaremos los mismos procedimientos que usamos con el árbol de decisión y el KNN, utilizando un RandomizedSearch en lugar del GridSearch para conseguir la mejor configuración de parámetros en nuestros clasificadores.\n",
    "\n",
    "Para comenzar, como siempre, lo primero que hacemos es importar los paquetes necesarios, e inicializar nuestra semilla para nuestros experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always load all scipy stack packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, integrate\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code configures matplotlib for proper rendering\n",
    "%matplotlib inline\n",
    "mpl.rcParams[\"figure.figsize\"] = \"8, 4\"\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=6342\n",
    "np.random.seed(6342)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargado de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, cargamos los datos de los datasets Pima y Wisconsin. Cada uno será separado en dos conjuntos, uno con los atributos y otro con la clase del dataset. Esto es necesario para que la libreria scikit learn pueda ser utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the file path to fit your system\n",
    "dfPima = pd.read_csv(\"../data/pima.csv\", dtype={ \"label\": 'category'})\n",
    "dfAttributesPima = dfPima.drop('label', 1)\n",
    "dfLabelPima = dfPima['label']\n",
    "\n",
    "dfWisc = pd.read_csv(\"../data/wisconsin.csv\", dtype={ \"label\": 'category'})\n",
    "dfAttributesWisc = dfWisc.drop('label', 1)\n",
    "dfLabelWisc = dfWisc['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargados los datasets, debemos dividirlos en dos conjuntos de Train y Test para poder realizar nuestros experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into train/test split for our experiments\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_attsPima, test_attsPima, train_labelPima, test_labelPima = train_test_split( \n",
    "    dfAttributesPima,\n",
    "    dfLabelPima,\n",
    "    test_size=0.2,\n",
    "    random_state=seed,\n",
    "    stratify=dfLabelPima)\n",
    "\n",
    "train_attsWisc, test_attsWisc, train_labelWisc, test_labelWisc = train_test_split( \n",
    "    dfAttributesWisc,\n",
    "    dfLabelWisc,\n",
    "    test_size=0.2,\n",
    "    random_state=seed,\n",
    "    stratify=dfLabelWisc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de los datos en dataset Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder realizar el tratamiento de valores perdidos en el dataset Pima, primero debemos seleccionar los atributos que contengan valores a cero que son considerados como perdidos. En este caso nosotros pensamos que un cero en la variable 'preg' no sería valor perdido, ya que los pacientes puede que no hayan estado embarazados ninguna vez, o que sean de género masculino. Por tanto, un cero en cualquiera de las otras variables predictoras será considerado como valor perdido, a excepción de la variable 'preg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['plas', 'pres', 'skin', 'insu', 'mass', 'pedi', 'age'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_ceros = dfPima.columns.drop([\"preg\",\"label\"])\n",
    "aux_ceros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez separadas las variables sobre las que queremos hacer tratamiento de valores perdidos, reemplazamos los valores a cero por valores de tipo NaN, para que podamos imputar estos valores por la media de las variables para nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in aux_ceros:\n",
    "    train_attsPima.replace({i: {0: np.nan}}, inplace = True)\n",
    "    train_labelPima.replace({i: {0: np.nan}}, inplace = True)\n",
    "    test_attsPima.replace({i: {0: np.nan}}, inplace = True)\n",
    "    test_labelPima.replace({i: {0: np.nan}}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de los datos en dataset Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el dataset Wisconsin, solamente prescindiremos de la variable 'patientId', como ya hicimos en la práctica anterior, porque esta variable es irrelevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attsWisc2=train_attsWisc.drop('patientId',1)\n",
    "test_attsWisc2=test_attsWisc.drop('patientId',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Selección y evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección realizaremos experimentos utilizando el algoritmo GridSearch de la librería scikit-learn para descubrir las configuraciones óptimas de los clasificadores DecisionTree y KNN para los datasets Pima y Wisconsin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pasos a seguir en este apartado son:\n",
    "* Crear un Pipeline con dos pasos:\n",
    "    - Un objeto de la clase Imputer que se encarga de realizar el tratamiento de valores perdidos. En nuestro caso, imputaremos los valores NaN de los datasets con la media.\n",
    "    - Un clasificador como estimador (árbol de decisión o KNN en nuestro caso).\n",
    "* Crear un objeto de la clase GridSearchCV, al que le pasaremos:\n",
    "    - el estimador que usamos en el Pipeline, \n",
    "    - los hiperparámetros del clasificador que queremos tener en cuenta a la hora de buscar la configuración óptima.\n",
    "    - También debemos indicar qué tipo de métrica usaremos para valorar las configuraciones,\n",
    "    - cuántos folds se deben realizar en el proceso de validación cruzada que realiza el GridSearchCV. Si en este campo especificamos un objeto de la clase StratifiedKFold con 10 folds y nuestra semilla, podrá realizar un proceso de validación cruzada estratificada.\n",
    "    - iid = False, para que la evaluación de los resultados se haga sobre una media aritmética, y no sobre una media ponderada.\n",
    "* Entrenar y validar el clasificador con los datos de nuestros datasets, y analizar los resultados obtenidos (accuracy, precision y recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Árbol de decisión con preprocesamiento de los datos durante la validación cruzada (transformers y pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos con el clasificador DecisionTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "# Cargamos el arbol de decision\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación creamos un Pipeline con dos pasos: el primero es imputar los valores perdidos siguiendo la estrategia de rellenar con la media, y el segundo paso es un clasificador DecisionTree, con su semilla para poder hacer reproducibles nuestros experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the pipeline as a set of tuples\n",
    "estimatorTree = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"Tree\", tree.DecisionTreeClassifier(random_state=seed))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí definimos nuestro GridSearch con los parámetros elegidos. Como estimador debemos pasarle el que usamos en nuestro Pipeline.\n",
    "\n",
    "En nuestro caso hemos elegido los parámetros 'criterion' (puede ser \"gini\", que usa la impureza Gini como función, o “entropy”, usando la ganancia de información), 'max_depth' (para indicar la máxima profundidad del árbol), y 'min_samples_leaf' (para indicar el número mínimo de ejemplos que queremos que haya en cada hoja del árbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfTree = GridSearchCV(\n",
    "    estimator = estimatorTree,\n",
    "    param_grid =\n",
    "        {'Tree__criterion': [\"entropy\",\"gini\"],'Tree__max_depth': [3, 5, 10, None], 'Tree__min_samples_leaf': [3,5,10]},\n",
    "    scoring = 'accuracy',\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed), iid=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si asignamos el valor 'None' al parámetro 'max_depth', el clasificador intentará llegar a la máxima profundidad del árbol por defecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de pasar el GridSearch, ejecutamos con el dataset Pima para entrenar el clasificador con el conjunto de Train, validamos con nuestro conjunto de Test, y después imprimimos el accuracy obtenido con la mejor configuración de parámetros del árbol encontrada por nuestro GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72727272727272729"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreePima = clfTree.fit(train_attsPima, train_labelPima)\n",
    "predictionTreePima = clfTree.predict(test_attsPima)\n",
    "print('Accuracy Pima Tree:')\n",
    "metrics.accuracy_score(test_labelPima, predictionTreePima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, nos parece interesante mostrar la mejor configuración de los parámetros del árbol que ha encontrado el GridSearch. En este caso, la mejor configuración es utilizar el criterio de entropía para evaluar las variables del árbol, emplear una profundidad máxima del árbol de 10, y un número mínimo de ejemplos por hoja igual a 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima Tree):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Tree__criterion': 'entropy',\n",
       " 'Tree__max_depth': 10,\n",
       " 'Tree__min_samples_leaf': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima Tree):')\n",
    "clsTreePima.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además del accuracy, también es interesante mostrar la matriz de confusión obtenida, y las medidas de precision y recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Pima Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[79, 21],\n",
       "       [21, 33]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Pima Tree:')\n",
    "metrics.confusion_matrix(test_labelPima, predictionTreePima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61111111111111116"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima Tree:')\n",
    "metrics.recall_score(test_labelPima, predictionTreePima, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61111111111111116"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima Tree:')\n",
    "metrics.precision_score(test_labelPima, predictionTreePima, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el mismo procedimiento con el dataset Wisconsin. Usando el Pipeline creado en el apartado 1.1.1., entrenamos, validamos y obtenemos las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95714285714285718"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreeWisc = clfTree.fit(train_attsWisc2, train_labelWisc)\n",
    "predictionTreeWisc = clfTree.predict(test_attsWisc2)\n",
    "print('Accuracy Wisconsin Tree:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionTreeWisc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora mostramos la mejor configuración obtenida con el GridSearch, que en este caso consiste en usar el criterio gini para el árbol, una profundidad máxima de 5, y un número mínimo de ejemplos por hoja igual a 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin Tree):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Tree__criterion': 'gini', 'Tree__max_depth': 5, 'Tree__min_samples_leaf': 5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin Tree):')\n",
    "clsTreeWisc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos la matriz de confusión, precision y recall, igual que hicimos con el dataset Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Wisconsin Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[90,  2],\n",
       "       [ 4, 44]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Wisconsin Tree:')\n",
    "metrics.confusion_matrix(test_labelWisc, predictionTreeWisc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91666666666666663"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin Tree:')\n",
    "metrics.recall_score(test_labelWisc, predictionTreeWisc, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95652173913043481"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin Tree:')\n",
    "metrics.precision_score(test_labelWisc, predictionTreeWisc, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. KNN con preprocesamiento de los datos durante la validación cruzada (transformers y pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte vamos a usar un KNN como clasificador para nuestro Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el Pipeline con el imputer como primer paso, y como segundo paso, el clasificador KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the pipeline as a set of tuples\n",
    "estimatorKNN = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"KNN\", neighbors.KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos los parámetros del algoritmo de KNN que queremos que el GridSearch tenga en cuenta a la hora de buscar la configuración óptima. En el algoritmo KNeighborsClassifier no se nos permite elegir un random_state como en el caso del árbol para poder hacer reproducibles nuestros experimentos, por lo que solo usaremos la semilla en el proceso de validación cruzada que se realiza en el GridSearch.\n",
    "\n",
    "En nuestro caso, para el KNN hemos añadido el número de vecinos, y el tipo de métrica a utilizar para valorar las distancias de los vecinos (puede ser la distancia o la inversa de la distancia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfKNN = GridSearchCV(\n",
    "    estimator = estimatorKNN,\n",
    "    param_grid = \n",
    "        { 'KNN__n_neighbors' : [1,2,3,4,5], 'KNN__weights': ['uniform','distance'] },\n",
    "    scoring = 'accuracy',\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed), iid=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero entrenamos, validamos y obtenemos el accuracy en los datos del dataset Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68831168831168832"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsKNNPima = clfKNN.fit(train_attsPima, train_labelPima)\n",
    "predictionKNNPima = clfKNN.predict(test_attsPima)\n",
    "print('Accuracy Pima KNN:')\n",
    "metrics.accuracy_score(test_labelPima, predictionKNNPima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos cuál ha sido la mejor configuración de parámetros obtenida por el GridSearch. En este caso es utilizar 5 vecinos, usando la métrica uniforme para medir las distancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima KNN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNN__n_neighbors': 5, 'KNN__weights': 'uniform'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima KNN):')\n",
    "clsKNNPima.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación mostramos la matriz de confusión, precision y recall obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Pima KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[82, 18],\n",
       "       [30, 24]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Pima KNN:')\n",
    "metrics.confusion_matrix(test_labelPima, predictionKNNPima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44444444444444442"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima KNN:')\n",
    "metrics.recall_score(test_labelPima, predictionKNNPima, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima KNN:')\n",
    "metrics.precision_score(test_labelPima, predictionKNNPima, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos del dataset Wisconsin, entrenamos nuestro clasificador, validamos y obtenemos el accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97857142857142854"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsKNNWisc = clfKNN.fit(train_attsWisc2, train_labelWisc)\n",
    "predictionKNNWisc = clfKNN.predict(test_attsWisc2)\n",
    "print('Accuracy Wisconsin KNN:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionKNNWisc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mejores parámetros obtenidos por el GridSearch para el KNN en este dataset son utilizar 3 vecinos, usando la métrica uniforme para medir las distancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin KNN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNN__n_neighbors': 3, 'KNN__weights': 'uniform'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin KNN):')\n",
    "clsKNNWisc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos la matriz de confusión, precision y recall obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Wisconsin KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[91,  1],\n",
       "       [ 2, 46]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Wisconsin KNN:')\n",
    "metrics.confusion_matrix(test_labelWisc, predictionKNNWisc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95833333333333337"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin KNN:')\n",
    "metrics.recall_score(test_labelWisc, predictionKNNWisc, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97872340425531912"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin KNN:')\n",
    "metrics.precision_score(test_labelWisc, predictionKNNWisc, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Comparativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado intentaremos analizar cuál de los clasificadores ha obtenido mejores resultados en los datasets estudiados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataset Pima los resultados de accuracy para el árbol de decisión y el KNN con k=5, son: \n",
    "* Accuracy Pima Tree: 0.72727272727272729\n",
    "* Accuracy Pima 5-NN: 0.68831168831168832\n",
    "\n",
    "En cuanto a los resultados de recall tenemos:\n",
    "* Recall Pima Tree: 0.61111111111111116\n",
    "* Recall Pima 5-NN: 0.44444444444444442\n",
    "\n",
    "Y los de precision:\n",
    "* Precision Pima Tree: 0.61111111111111116\n",
    "* Precision Pima 5-NN: 0.5714285714285714\n",
    "\n",
    "Por tanto, podemos concluir que el mejor modelo es el obtenido con el árbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataset Wisconsin, los resultados de acurracy para el árbol de decisión y el KNN con k=3 son:\n",
    "* Accuracy Wisconsin Tree: 0.95714285714285718\n",
    "* Accuracy Wisconsin 3-NN: 0.97857142857142854\n",
    "\n",
    "Los resultados de recall son:\n",
    "* Recall Wisconsin Tree: 0.91666666666666663\n",
    "* Recall Wisconsin 3-NN: 0.95833333333333337\n",
    "\n",
    "Y los de precision:\n",
    "* Precision Wisconsin Tree: 0.95652173913043481\n",
    "* Precision Wisconsin 3-NN: 0.97872340425531912\n",
    "\n",
    "Como vemos, el mejor modelo para este dataset podría ser el KNN en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Estudio de los algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado vamos a estudiar invidualmente los algoritmos:\n",
    "* En KNN, estudiaremos los parámetros aprendidos por el clasificador, realizando varias pruebas con distintas configuraciones de parámetros y analizando los resultados. Analizaremos por separado los dos datasets.\n",
    "* Para el árbol, estudiaremos los parámetros aprendidos por el clasificador, y también su estructura. Analizaremos las estructuras del clasificador con la configuración óptima y otra con una configuración suboptima, y valoraremos los resultados.\n",
    "\n",
    "Para analizarlos, primero vamos a crear una funcion getScores que será similar a nuestra implementación del GridSearch pero más sencilla. Simplemente obtendremos los scores usando los distintos parámetros del clasificador. No realizamos proceso de validación cruzada, solamente usamos el holdout que teníamos al principio de la práctica.\n",
    "\n",
    "Después obtenemos tiempo y accuracy para las configuraciones, y realizaremos una comparativa de todos ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from time import time\n",
    "\n",
    "def getScores(estim,paramG,train_atts,train_label,test_atts,test_label):\n",
    "    \n",
    "    scores=[] #para almacenar los resultados de accuracy a devolver\n",
    "    exTimes=[] #para almacenar los tiempos a devolver\n",
    "    \n",
    "    #Generamos las configuraciones de parametros como en GridS\n",
    "    items = paramG.items()\n",
    "    keys, values = zip(*items)\n",
    "    v = list(product(*values))\n",
    "    \n",
    "    # Recorremos las configuraciones y almacenamos los resultados\n",
    "    for param in v:\n",
    "        \n",
    "        params = dict(zip(keys,param))\n",
    "        estim.set_params(**params)\n",
    "        \n",
    "        timeSum=0 #variable intermedia para mostrar la media de los tiempos\n",
    "        \n",
    "        for i in range(0,100):\n",
    "            start = time()\n",
    "            estim.fit(train_atts,train_label) #entrenamos con el conjunto de Train\n",
    "            predictions = estim.predict(test_atts) #array de predicciones para el conjunto de Test\n",
    "            end = time()\n",
    "            timeSum += (end - start)\n",
    "        exTimes.append(timeSum/100) #añadimos al array de tiempos la media de las 100 medidas de tiempo realizadas\n",
    "        \n",
    "        comparison = np.sum(predictions == test_label) #comparamos predicciones con test_label y sumamos los aciertos\n",
    "        accuracy = comparison / len(predictions) #dividimos entre los casos totales\n",
    "        scores.append(accuracy) #añadimos al array de scores el accuracy obtenido con la configuración de parámetros actual\n",
    "        \n",
    "    return (v,scores,exTimes) #devolvemos las configuraciones, los accuracys y los tiempos de cada configuracion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Estudio del algoritmo KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este clasificador, usaremos las combinaciones teniendo en cuenta el número de vecinos (k=1,5,10,50,100) y el método de evaluación de las distancias (uniforme o inversa de la distancia). Para ello usaremos nuestra función getScores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.1. KNN con Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero de todo es imputar con la media los valores perdidos del dataset Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp=Imputer(missing_values='NaN', strategy=\"mean\",axis=0)\n",
    "#we fit it\n",
    "impPima = imp.fit(train_attsPima)\n",
    "# Finally we can use it to transform any dataframe:\n",
    "train_attsPima_clean = impPima.transform(train_attsPima)\n",
    "test_attsPima_clean = impPima.transform(test_attsPima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecho esto, podemos hacer una comparativa de los resultados obtenidos para todas las configuraciones del clasificador KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paramsKNNPima</th>\n",
       "      <th>scoresKNNPima</th>\n",
       "      <th>exTimesKNNPima</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(uniform, 1)</td>\n",
       "      <td>0.629870</td>\n",
       "      <td>0.002003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(uniform, 5)</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.002124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(uniform, 10)</td>\n",
       "      <td>0.694805</td>\n",
       "      <td>0.002428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(uniform, 50)</td>\n",
       "      <td>0.733766</td>\n",
       "      <td>0.003427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(uniform, 100)</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.004955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(distance, 1)</td>\n",
       "      <td>0.629870</td>\n",
       "      <td>0.001996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(distance, 5)</td>\n",
       "      <td>0.694805</td>\n",
       "      <td>0.002142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(distance, 10)</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.002342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(distance, 50)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.003641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(distance, 100)</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.005281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paramsKNNPima  scoresKNNPima  exTimesKNNPima\n",
       "0     (uniform, 1)       0.629870        0.002003\n",
       "1     (uniform, 5)       0.688312        0.002124\n",
       "2    (uniform, 10)       0.694805        0.002428\n",
       "3    (uniform, 50)       0.733766        0.003427\n",
       "4   (uniform, 100)       0.707792        0.004955\n",
       "5    (distance, 1)       0.629870        0.001996\n",
       "6    (distance, 5)       0.694805        0.002142\n",
       "7   (distance, 10)       0.701299        0.002342\n",
       "8   (distance, 50)       0.746753        0.003641\n",
       "9  (distance, 100)       0.727273        0.005281"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsKNNPima,scoresKNNPima,exTimesKNNPima=getScores(neighbors.KNeighborsClassifier(),\n",
    "                                                     { 'weights': ['uniform','distance'], 'n_neighbors' : [1,5,10,50,100] },\n",
    "                                                     train_attsPima_clean,train_labelPima,\n",
    "                                                     test_attsPima_clean,test_labelPima)\n",
    "resultadosKNNPima=pd.DataFrame(list(zip(paramsKNNPima,scoresKNNPima,exTimesKNNPima)))\n",
    "resultadosKNNPima.columns=['paramsKNNPima','scoresKNNPima','exTimesKNNPima']\n",
    "resultadosKNNPima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos comprobar, la configuración con mejor accuracy sería usar k=50 y la inversa de la distancia (fila 8 de la tabla). Por tanto, si aumentamos K, podríamos pensar que se mejora el accuracy, pero solo hasta cierto punto, porque con K=100 (fila 9) ya obtenemos un accuracy peor. De forma general, usar un K muy alto se asemeja a un clasificador ZeroR, ya que ambos utilizarían la estrategia de clasificación por la clase mayoritaria.\n",
    "\n",
    "Además, debemos tener en mente que cuanto mayor sea el parámetro K, mayor será el tiempo de ejecución. En este caso, el tiempo de ejecución se corresponde con el tiempo de realizar la predicción con el conjunto de Test. Esto es debido a que el entrenamiento en el clasificador KNN es muy rápido, ya que solo consiste en copiar la base de datos de los casos de Train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.2. KNN con Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estudiaremos las configuraciones obtenidas con el dataset Wisconsin. Primero debemos imputar los valores perdidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we fit it\n",
    "impWisc = imp.fit(train_attsWisc2)\n",
    "# Finally we can use it to transform any dataframe:\n",
    "train_attsWisc2_clean = impWisc.transform(train_attsWisc2)\n",
    "test_attsWisc2_clean = impWisc.transform(test_attsWisc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecho esto, realizamos la comparativa de los resultados obtenidos para todas las configuraciones del clasificador KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paramsKNNWisc</th>\n",
       "      <th>scoresKNNWisc</th>\n",
       "      <th>exTimesKNNWisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(uniform, 1)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.002401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(uniform, 5)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.002179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(uniform, 10)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.002381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(uniform, 50)</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.003233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(uniform, 100)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.004371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(distance, 1)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.002104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(distance, 5)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.002229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(distance, 10)</td>\n",
       "      <td>0.978571</td>\n",
       "      <td>0.002346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(distance, 50)</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.003307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(distance, 100)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.004571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paramsKNNWisc  scoresKNNWisc  exTimesKNNWisc\n",
       "0     (uniform, 1)       0.964286        0.002401\n",
       "1     (uniform, 5)       0.964286        0.002179\n",
       "2    (uniform, 10)       0.964286        0.002381\n",
       "3    (uniform, 50)       0.971429        0.003233\n",
       "4   (uniform, 100)       0.964286        0.004371\n",
       "5    (distance, 1)       0.964286        0.002104\n",
       "6    (distance, 5)       0.964286        0.002229\n",
       "7   (distance, 10)       0.978571        0.002346\n",
       "8   (distance, 50)       0.971429        0.003307\n",
       "9  (distance, 100)       0.964286        0.004571"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsKNNWisc,scoresKNNWisc,exTimesKNNWisc=getScores(neighbors.KNeighborsClassifier(),\n",
    "                                                     { 'weights': ['uniform','distance'], 'n_neighbors' : [1,5,10,50,100] },\n",
    "                                                     train_attsWisc2_clean,train_labelWisc,\n",
    "                                                     test_attsWisc2_clean,test_labelWisc)\n",
    "resultadosKNNWisc=pd.DataFrame(list(zip(paramsKNNWisc,scoresKNNWisc,exTimesKNNWisc)))\n",
    "resultadosKNNWisc.columns=['paramsKNNWisc','scoresKNNWisc','exTimesKNNWisc']\n",
    "resultadosKNNWisc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos comprobar, la configuración con mejor accuracy sería usar k=10 y la inversa de la distancia (fila 7 de la tabla). \n",
    "\n",
    "Con este dataset, a diferencia de lo que observamos con Pima, vemos que la mejor opción es usar valores de K algo más pequeños, ya que los accuracys empeoran al usar K's más grandes. Además, cuanto mayor es K, mayor será el tiempo de ejecución del algoritmo. \n",
    "\n",
    "Como ya mencionamos anteriormente, de forma general podemos decir que usar un K muy alto en KNN se asemeja a un clasificador ZeroR, ya que ambos utilizarían la estrategia de clasificación por la clase mayoritaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2. Estudio del algoritmo DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este clasificador, realizaremos las combinaciones teniendo en cuenta el criterio (entropía o gini), la profundidad máxima del árbol, y el número mínimo de ejemplos por hoja. \n",
    "\n",
    "Para analizar la estructura de los árboles generados con las diferentes combinaciones de parámetros, vamos a usar (además de nuestra función getScores) la función getTreeNodes, que nos devuelve el número de nodos que presenta un árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTreeNodes(estim,paramG,train_atts,train_label):\n",
    "    \n",
    "    nodes=[] #para almacenar el numero de nodos de cada arbol generado\n",
    "    \n",
    "    #Generamos las configuraciones de parametros como en GridS\n",
    "    items = paramG.items()\n",
    "    keys, values = zip(*items)\n",
    "    v = list(product(*values))\n",
    "    \n",
    "    # Recorremos las configuraciones y almacenamos los resultados\n",
    "    for param in v:\n",
    "        \n",
    "        params = dict(zip(keys,param))\n",
    "        estim.set_params(**params)\n",
    "\n",
    "        estim.fit(train_atts,train_label) #entrenamos con el conjunto de Train\n",
    "        \n",
    "        nodes.append(estim.tree_.node_count) #añadimos al array el numero de nodos del arbol actual\n",
    "        \n",
    "    return (v,nodes) #devolvemos las configuraciones de los arboles y el numero de nodos de cada uno de ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.1. Árbol con Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzaremos con el dataset Pima. Vamos a hacer una comparativa de los resultados obtenidos para todas las configuraciones del árbol de clasificación, junto con la complejidad de cada árbol generado (representada por el número de nodos de cada árbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsTreePima,scoresTreePima,exTimesTreePima=getScores(tree.DecisionTreeClassifier(random_state=seed),\n",
    "                                                        {'criterion': [\"entropy\",\"gini\"],\n",
    "                                                         'max_depth': [3, 5, 10], \n",
    "                                                         'min_samples_leaf': [3,5,10]},\n",
    "                                                        train_attsPima_clean,train_labelPima,\n",
    "                                                        test_attsPima_clean,test_labelPima)\n",
    "resultadosTreePima=pd.DataFrame(list(zip(paramsTreePima,scoresTreePima,exTimesTreePima)))\n",
    "resultadosTreePima.columns=['paramsTreePima','scoresTreePima','exTimesTreePima']\n",
    "#resultadosTreePima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paramsTreePima</th>\n",
       "      <th>scoresTreePima</th>\n",
       "      <th>exTimesTreePima</th>\n",
       "      <th>nodesTreePima</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(entropy, 3, 3)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(entropy, 3, 5)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(entropy, 3, 10)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(entropy, 5, 3)</td>\n",
       "      <td>0.720779</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(entropy, 5, 5)</td>\n",
       "      <td>0.720779</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(entropy, 5, 10)</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(entropy, 10, 3)</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(entropy, 10, 5)</td>\n",
       "      <td>0.720779</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(entropy, 10, 10)</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(gini, 3, 3)</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(gini, 3, 5)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(gini, 3, 10)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(gini, 5, 3)</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(gini, 5, 5)</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(gini, 5, 10)</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(gini, 10, 3)</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(gini, 10, 5)</td>\n",
       "      <td>0.759740</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(gini, 10, 10)</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paramsTreePima  scoresTreePima  exTimesTreePima  nodesTreePima\n",
       "0     (entropy, 3, 3)        0.746753         0.002374             15\n",
       "1     (entropy, 3, 5)        0.746753         0.002088             15\n",
       "2    (entropy, 3, 10)        0.746753         0.002269             15\n",
       "3     (entropy, 5, 3)        0.720779         0.003040             39\n",
       "4     (entropy, 5, 5)        0.720779         0.003044             41\n",
       "5    (entropy, 5, 10)        0.707792         0.002818             43\n",
       "6    (entropy, 10, 3)        0.727273         0.004216            131\n",
       "7    (entropy, 10, 5)        0.720779         0.003831            111\n",
       "8   (entropy, 10, 10)        0.727273         0.003169             73\n",
       "9        (gini, 3, 3)        0.753247         0.001406             15\n",
       "10       (gini, 3, 5)        0.746753         0.001399             15\n",
       "11      (gini, 3, 10)        0.746753         0.001407             15\n",
       "12       (gini, 5, 3)        0.714286         0.001865             41\n",
       "13       (gini, 5, 5)        0.707792         0.001853             41\n",
       "14      (gini, 5, 10)        0.714286         0.001925             39\n",
       "15      (gini, 10, 3)        0.740260         0.002473            139\n",
       "16      (gini, 10, 5)        0.759740         0.002330            107\n",
       "17     (gini, 10, 10)        0.707792         0.002059             69"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsTreePima,nodesTreePima=getTreeNodes(tree.DecisionTreeClassifier(random_state=seed),\n",
    "                                          {'criterion': [\"entropy\",\"gini\"],\n",
    "                                           'max_depth': [3, 5, 10], \n",
    "                                           'min_samples_leaf': [3,5,10]},\n",
    "                                          train_attsPima_clean,train_labelPima)\n",
    "resultadosTreePima.assign(nodesTreePima = nodesTreePima)\n",
    "#resultadosTreePima['nodesTreePima']=nodesTreePima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos en la tabla, la configuración con mayor accuracy sería la que se encuentra en la fila 16: criterio gini, máxima profundidad del árbol igual a 10 y mínimo número de ejemplos por hoja igual a 5.\n",
    "\n",
    "Sin embargo, esta configuración genera un árbol muy complejo, con más de 100 nodos. Por tanto, podríamos compararlo con el segundo mejor árbol generado (árbol de la fila 9 de la tabla), el cual presenta un accuracy algo más bajo, pero está compuesto por tan solo 15 nodos. Este caso necesitaría una profundidad máxima del árbol igual a 3, y un número mínimo de ejemplos por hoja igual a 3, para conseguir (casi) igualar el accuracy al del árbol de la fila 16, por lo que sería interesante seleccionar el árbol más simple para nuestra clasificación.\n",
    "\n",
    "De esta forma podemos ver que debemos tener en cuenta otros factores además del accuracy para poder evaluar la efectividad del clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.2. Árbol con Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora haremos lo mismo con los árboles generados para el dataset Wisconsin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsTreeWisc,scoresTreeWisc,exTimesTreeWisc=getScores(tree.DecisionTreeClassifier(random_state=seed),\n",
    "                                                        {'criterion': [\"entropy\",\"gini\"],\n",
    "                                                         'max_depth': [3, 5, 10], \n",
    "                                                         'min_samples_leaf': [3,5,10]},\n",
    "                                                        train_attsWisc2_clean,train_labelWisc,\n",
    "                                                        test_attsWisc2_clean,test_labelWisc)\n",
    "resultadosTreeWisc=pd.DataFrame(list(zip(paramsTreeWisc,scoresTreeWisc,exTimesTreeWisc)))\n",
    "resultadosTreeWisc.columns=['paramsTreeWisc','scoresTreeWisc','exTimesTreeWisc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paramsTreeWisc</th>\n",
       "      <th>scoresTreeWisc</th>\n",
       "      <th>exTimesTreeWisc</th>\n",
       "      <th>nodesTreeWisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(entropy, 3, 3)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(entropy, 3, 5)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(entropy, 3, 10)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(entropy, 5, 3)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(entropy, 5, 5)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(entropy, 5, 10)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(entropy, 10, 3)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(entropy, 10, 5)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(entropy, 10, 10)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(gini, 3, 3)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(gini, 3, 5)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(gini, 3, 10)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(gini, 5, 3)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(gini, 5, 5)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(gini, 5, 10)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(gini, 10, 3)</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(gini, 10, 5)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(gini, 10, 10)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paramsTreeWisc  scoresTreeWisc  exTimesTreeWisc  nodesTreeWisc\n",
       "0     (entropy, 3, 3)        0.950000         0.001479             15\n",
       "1     (entropy, 3, 5)        0.950000         0.001279             15\n",
       "2    (entropy, 3, 10)        0.950000         0.000999             15\n",
       "3     (entropy, 5, 3)        0.950000         0.001119             35\n",
       "4     (entropy, 5, 5)        0.964286         0.001070             31\n",
       "5    (entropy, 5, 10)        0.950000         0.001018             25\n",
       "6    (entropy, 10, 3)        0.964286         0.001103             41\n",
       "7    (entropy, 10, 5)        0.964286         0.001070             33\n",
       "8   (entropy, 10, 10)        0.950000         0.001013             25\n",
       "9        (gini, 3, 3)        0.964286         0.000933             15\n",
       "10       (gini, 3, 5)        0.957143         0.001097             15\n",
       "11      (gini, 3, 10)        0.957143         0.000910             13\n",
       "12       (gini, 5, 3)        0.950000         0.001087             35\n",
       "13       (gini, 5, 5)        0.957143         0.001054             33\n",
       "14      (gini, 5, 10)        0.957143         0.001019             23\n",
       "15      (gini, 10, 3)        0.942857         0.001089             45\n",
       "16      (gini, 10, 5)        0.957143         0.001049             35\n",
       "17     (gini, 10, 10)        0.957143         0.001007             23"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsTreeWisc,nodesTreeWisc=getTreeNodes(tree.DecisionTreeClassifier(random_state=seed),\n",
    "                                          {'criterion': [\"entropy\",\"gini\"],\n",
    "                                           'max_depth': [3, 5, 10], \n",
    "                                           'min_samples_leaf': [3,5,10]},\n",
    "                                          train_attsWisc2_clean,train_labelWisc)\n",
    "resultadosTreeWisc.assign(nodesTreeWisc = nodesTreeWisc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos en la tabla, una de las configuraciones con mayor accuracy sería la que se encuentra en la fila 4: criterio entropía, máxima profundidad del árbol igual a 5 y mínimo número de ejemplos por hoja igual a 5.\n",
    "\n",
    "En este caso, los árboles no son demasiado complejos, todos tienen un número de nodos similar. Aún así podemos observar algunas pequeñas diferencias en dicho número.\n",
    "\n",
    "Una configuración subóptima que sería interesante analizar es la que se encuentra en la fila 9 (criterio gini, máxima profundidad de 3 y mínimo número de ejemplos por hoja igual a 3), ya que obtiene un accuracy igual al árbol de la fila 4, pero presenta un número de nodos menor (se reduce a la mitad). Por ello, podría resultar interesante seleccionar el árbol más simple para nuestra clasificación.\n",
    "\n",
    "De esta forma, al igual que en el dataset Pima, llegamos a la conclusión de que debemos tener en cuenta otros factores además del accuracy para poder evaluar la efectividad del clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3. Conclusiones del estudio de los algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras el estudio de los algoritmos, nos hemos dado cuenta de que dependiendo del problema y del dataset al que nos enfrentemos, es posible que la configuración de parámetros óptima para un determinado algoritmo no siempre sea la misma. \n",
    "* En el KNN con el dataset Pima hemos visto que es bueno aumentar la vecindad (hasta cierto punto). Sin embargo, con el dataset Wisconsin los resultados empeoran al usar vecindades grandes, y además se consumía demasiado tiempo.\n",
    "* Con el árbol de decisión, hemos aprendido que es una buena práctica observar las configuraciones subóptimas del clasificador para obtener árboles más sencillos, en los cuales el tiempo de ejecución es menor y obtienen rendimientos similares a árboles más grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementación de GridSearch manualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado realizamos una implementación manual de un GridSearh básico, con su constructor, y las funciones fit y predict, que son las que utilizamos en esta práctica.\n",
    "\n",
    "Nuestra clase GridS tiene como atributos:\n",
    "* estim: estimador o clasificador a evaluar,\n",
    "* crossval: cómo realizar el proceso de validación cruzada durante la evaluación,\n",
    "* score: tipo de métrica a utilizar para la evaluación de la configuración óptima,\n",
    "* keys: nombre de los parámetros del clasificador a tener en cuenta en la evaluación,\n",
    "* v: lista con todas las posibles combinaciones de parámetros del clasificador,\n",
    "* bestScore: mejor puntuación obtenida con la configuración de parámetros del clasificador,\n",
    "* bestParams: mejor configuración de parámetros del clasificador.\n",
    "\n",
    "La función fit se encarga de realizar la validación cruzada para cada una de las diferentes combinaciones de parámetros contenidas en self.v, obteniendo así aquella configuración que ofrezca una mayor puntuación. Una vez obtenida, entrena el clasificador con el conjunto de datos que se le pasa como parámetro a la función fit.\n",
    "\n",
    "La función predict simplemente hace la validación del clasificador con el conjunto que reciba como parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "\n",
    "class GridS(object):\n",
    "    def __init__(self,estim, paramG, score, crossval):\n",
    "        items = paramG.items()\n",
    "        self.keys, values = zip(*items)\n",
    "        self.v = list(product(*values))\n",
    "        self.estim=estim\n",
    "        self.crossval=crossval\n",
    "        self.score=score\n",
    "        self.bestParams = []\n",
    "            \n",
    "    def fit(self,atts,label):\n",
    "        self.bestScore=0\n",
    "        for param in self.v:\n",
    "            params = dict(zip(self.keys,param))\n",
    "            self.estim.set_params(**params)\n",
    "            scores = cross_val_score(estimator=self.estim, \n",
    "                                     X=atts, y=label, \n",
    "                                     scoring=self.score, \n",
    "                                     cv=self.crossval)\n",
    "            if self.bestScore < scores.mean():\n",
    "                self.bestScore=scores.mean()\n",
    "                self.bestParams=params\n",
    "        self.estim.set_params(**self.bestParams)\n",
    "        self.estim.fit(atts,label)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, atts):\n",
    "        return self.estim.predict(atts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. KNN con GridSearch manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un Pipeline que tiene como primer paso el Imputer de los datos, y como segundo paso el clasificador KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorGSKNN = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"myKNN\", neighbors.KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que hicimos anteriormente con el GridSearch de scikit, creamos un objeto de la clase GridS. Le pasamos nuestro pipeline, y le indicamos que obtenga la mejor configuración de parámetros atendiendo al número de vecinos (de 1 a 5 vecinos), y el tipo de métrica a utilizar para valorar la distancia (uniforme o inversa de la distancia). El tipo de métrica a utilizar será de nuevo el accuracy, y utilizaremos un proceso de validación cruzada estratificada de 10 folds, con nuestra semilla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsKNN = GridS(\n",
    "    estim = estimatorGSKNN,\n",
    "    paramG = \n",
    "        { 'myKNN__n_neighbors' : [1,2,3,4,5], 'myKNN__weights': ['uniform','distance'] },\n",
    "    score = 'accuracy',\n",
    "    crossval = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación utilizamos nuestro GridS en el dataset Pima y evaluamos los resultados. Como vemos, son los mismos que obtuvimos usando el GridSearch de scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68831168831168832"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clsKNNPimaGS = gsKNN.fit(train_attsPima, train_labelPima)\n",
    "predictionKNNPimaGS = gsKNN.predict(test_attsPima)\n",
    "print('Accuracy Pima KNN + GridS:')\n",
    "metrics.accuracy_score(test_labelPima, predictionKNNPimaGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima KNN + GridS):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'myKNN__n_neighbors': 5, 'myKNN__weights': 'uniform'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima KNN + GridS):')\n",
    "clsKNNPimaGS.bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima KNN + GridS:')\n",
    "metrics.precision_score(test_labelPima, predictionKNNPimaGS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44444444444444442"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima KNN + GridS:')\n",
    "metrics.recall_score(test_labelPima, predictionKNNPimaGS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos lo mismo con el dataset Wisconsin. Como vemos, se obtienen los mismos resultados que teníamos usando el GridSearch de scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wiscconsin KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97857142857142854"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clsKNNWiscGS = gsKNN.fit(train_attsWisc2, train_labelWisc)\n",
    "predictionKNNWiscGS = gsKNN.predict(test_attsWisc2)\n",
    "print('Accuracy Wiscconsin KNN + GridS:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionKNNWiscGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin KNN + GridS):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'myKNN__n_neighbors': 3, 'myKNN__weights': 'uniform'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin KNN + GridS):')\n",
    "clsKNNWiscGS.bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97872340425531912"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin KNN + GridS:')\n",
    "metrics.precision_score(test_labelWisc, predictionKNNWiscGS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95833333333333337"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin KNN + GridS:')\n",
    "metrics.recall_score(test_labelWisc, predictionKNNWiscGS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Árbol de decisión con GridSearch manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que hicimos anteriormente con el GridSearch de scikit, creamos un objeto de la clase GridS. Le pasamos un clasificador DecisionTree, y le indicamos que obtenga la mejor configuración de parámetros atendiendo al criterio de evaluación (entropía o gini), la máxima profundidad del árbol, y el número mínimo de ejemplos por hoja. El tipo de métrica a utilizar será de nuevo el accuracy, y utilizaremos un proceso de validación cruzada estratificada de 10 folds, con nuestra semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorGSTree = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"myTree\", tree.DecisionTreeClassifier(random_state=seed),)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsTree=GridS(\n",
    "    estim = estimatorGSTree,\n",
    "    paramG=\n",
    "        {'myTree__criterion': [\"entropy\",\"gini\"],'myTree__max_depth': [3, 5, 10, None], 'myTree__min_samples_leaf': [3,5,10]},\n",
    "    score='accuracy',\n",
    "    crossval=StratifiedKFold(n_splits=10, shuffle=False, random_state=seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación utilizamos nuestro GridS en el dataset Pima y evaluamos los resultados. Como vemos, son los mismos que obtuvimos usando el GridSearch de scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72727272727272729"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreePimaGS = gsTree.fit(train_attsPima, train_labelPima)\n",
    "predictionTreePimaGS = gsTree.predict(test_attsPima)\n",
    "print('Accuracy Pima Tree + GridS:')\n",
    "metrics.accuracy_score(test_labelPima, predictionTreePimaGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima Tree + GridS):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'myTree__criterion': 'entropy',\n",
       " 'myTree__max_depth': 10,\n",
       " 'myTree__min_samples_leaf': 10}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima Tree + GridS):')\n",
    "clsTreePimaGS.bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61111111111111116"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima Tree + GridS:')\n",
    "metrics.precision_score(test_labelPima, predictionTreePimaGS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61111111111111116"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima Tree + GridS:')\n",
    "metrics.recall_score(test_labelPima, predictionTreePimaGS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos lo mismo con el dataset Wisconsin. Como vemos, se obtienen los mismos resultados que teníamos usando el GridSearch de scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95714285714285718"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreeWiscGS = gsTree.fit(train_attsWisc2, train_labelWisc)\n",
    "predictionTreeWiscGS = gsTree.predict(test_attsWisc2)\n",
    "print('Accuracy Wisconsin Tree + GridS:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionTreeWiscGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin Tree + GridS):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'myTree__criterion': 'gini',\n",
       " 'myTree__max_depth': 5,\n",
       " 'myTree__min_samples_leaf': 5}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin Tree + GridS):')\n",
    "clsTreeWiscGS.bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95652173913043481"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin Tree + GridS:')\n",
    "metrics.precision_score(test_labelWisc, predictionTreeWiscGS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91666666666666663"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin Tree + GridS:')\n",
    "metrics.recall_score(test_labelWisc, predictionTreeWiscGS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Estudio de un kernel de kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extras: Algoritmo RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección realizaremos experimentos utilizando el algoritmo RandomizedSearch de la librería scikit-learn para descubrir las configuraciones óptimas de los clasificadores DecisionTree y KNN para los datasets Pima y Wisconsin. Seguiremos el mismo proceso que hicimos con GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Árbol de decisión con preprocesamiento de los datos durante la validación cruzada (transformers y pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos un Pipeline con dos pasos: el primero es imputar los valores perdidos siguiendo la estrategia de rellenar con la media, y el segundo paso es un árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the pipeline as a set of tuples\n",
    "estimatorTreeRS = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"RSTree\", tree.DecisionTreeClassifier(random_state=seed))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. RandomizedSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí definimos nuestro RandomizedSearch con los parámetros elegidos, usando nuestro Pipeline.\n",
    "\n",
    "En nuestro caso hemos elegido los parámetros 'criterion' (puede ser \"gini\", que usa la impureza Gini como función, o “entropy”, usando la ganancia de información), 'max_depth' (para indicar la máxima profundidad del árbol), y 'min_samples_leaf' (para indicar el número mínimo de ejemplos que queremos que haya en cada hoja del árbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfTreeRS = RandomizedSearchCV(\n",
    "    estimator = estimatorTreeRS,\n",
    "    param_distributions =\n",
    "        {'RSTree__criterion': [\"entropy\",\"gini\"],'RSTree__max_depth': [3, 5, 10, None], 'RSTree__min_samples_leaf': [3,5,10]},\n",
    "    scoring = 'accuracy',\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed), \n",
    "    iid=False,\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de crear el Pipeline, lo ejecutamos con el dataset Pima para entrenar el clasificador con el conjunto de Train, validamos con nuestro conjunto de Test, y después imprimimos el accuracy obtenido con la mejor configuración de parámetros del árbol encontrada por nuestro RandomizedSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72727272727272729"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreePimaRS = clfTreeRS.fit(train_attsPima, train_labelPima)\n",
    "predictionTreePimaRS = clfTreeRS.predict(test_attsPima)\n",
    "print('Accuracy Pima Tree with RS:')\n",
    "metrics.accuracy_score(test_labelPima, predictionTreePima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, nos parece interesante mostrar la mejor configuración de los parámetros del árbol que ha encontrado el RandomizedSearch. En este caso, la mejor configuración es utilizar el criterio de entropía para evaluar las variables del árbol, emplear una profundidad máxima del árbol de 3, y un número mínimo de ejemplos por hoja igual a 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima Tree) con RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RSTree__criterion': 'entropy',\n",
       " 'RSTree__max_depth': 3,\n",
       " 'RSTree__min_samples_leaf': 10}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima Tree) con RS:')\n",
    "clsTreePimaRS.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además del accuracy, también es interesante mostrar la matriz de confusión obtenida, y las medidas de precision y recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Pima Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85, 15],\n",
       "       [24, 30]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Pima Tree with RS:')\n",
    "metrics.confusion_matrix(test_labelPima, predictionTreePimaRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55555555555555558"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima Tree with RS:')\n",
    "metrics.recall_score(test_labelPima, predictionTreePimaRS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66666666666666663"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima Tree with RS:')\n",
    "metrics.precision_score(test_labelPima, predictionTreePimaRS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el mismo procedimiento con el dataset Wisconsin. Usando el Pipeline creado en el apartado 4.1.2., entrenamos, validamos y obtenemos las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95714285714285718"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreeWiscRS = clfTreeRS.fit(train_attsWisc2, train_labelWisc)\n",
    "predictionTreeWiscRS = clfTreeRS.predict(test_attsWisc2)\n",
    "print('Accuracy Wisconsin Tree with RS:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionTreeWiscRS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora mostramos la mejor configuración obtenida con el RandomizedSearch, que en este caso consiste en usar el criterio gini para el árbol, una profundidad máxima de 5, y un número mínimo de ejemplos por hoja igual a 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin Tree) con RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RSTree__criterion': 'gini',\n",
       " 'RSTree__max_depth': 10,\n",
       " 'RSTree__min_samples_leaf': 5}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin Tree) con RS:')\n",
    "clsTreeWiscRS.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos la matriz de confusión, precision y recall, igual que hicimos con el dataset Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Wisconsin Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[90,  2],\n",
       "       [ 4, 44]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Wisconsin Tree with RS:')\n",
    "metrics.confusion_matrix(test_labelWisc, predictionTreeWiscRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91666666666666663"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin Tree with RS:')\n",
    "metrics.recall_score(test_labelWisc, predictionTreeWiscRS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95652173913043481"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin Tree with RS:')\n",
    "metrics.precision_score(test_labelWisc, predictionTreeWiscRS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. KNN con preprocesamiento de los datos durante la validación cruzada (transformers y pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte vamos a usar un KNN como clasificador para nuestro Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el Pipeline con el imputer como primer paso, y como segundo paso, el clasificador KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the pipeline as a set of tuples\n",
    "estimatorKNNRS = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"RSKNN\", neighbors.KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. RandomizedSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos los parámetros del algoritmo de KNN que queremos que el RandomizedSearch tenga en cuenta a la hora de buscar la configuración óptima. En el algoritmo KNeighborsClassifier no se nos permite elegir un random_state como en el caso del árbol para poder hacer reproducibles nuestros experimentos, por lo que solo usaremos la semilla en el proceso de validación cruzada que se realiza en el RandomizedSearch.\n",
    "\n",
    "En nuestro caso, para el KNN hemos añadido el número de vecinos, y el tipo de métrica a utilizar para valorar las distancias de los vecinos (puede ser la distancia o la inversa de la distancia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfKNNRS = RandomizedSearchCV(\n",
    "    estimator = estimatorKNNRS,\n",
    "    param_distributions = \n",
    "        { 'RSKNN__n_neighbors' : [1,2,3,4,5], 'RSKNN__weights': ['uniform','distance'] },\n",
    "    scoring = 'accuracy',\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed), \n",
    "    iid=False,\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero entrenamos, validamos y obtenemos el accuracy en los datos del dataset Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68831168831168832"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsKNNPimaRS = clfKNNRS.fit(train_attsPima, train_labelPima)\n",
    "predictionKNNPimaRS = clfKNNRS.predict(test_attsPima)\n",
    "print('Accuracy Pima KNN with RS:')\n",
    "metrics.accuracy_score(test_labelPima, predictionKNNPimaRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos cuál ha sido la mejor configuración de parámetros obtenida por el RandomizedSearch. En este caso es utilizar 5 vecinos, usando la métrica uniforme para medir las distancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima KNN) con RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RSKNN__n_neighbors': 5, 'RSKNN__weights': 'uniform'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima KNN) con RS:')\n",
    "clsKNNPimaRS.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación mostramos la matriz de confusión, precision y recall obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Pima KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[82, 18],\n",
       "       [30, 24]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Pima KNN with RS:')\n",
    "metrics.confusion_matrix(test_labelPima, predictionKNNPimaRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44444444444444442"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima KNN with RS:')\n",
    "metrics.recall_score(test_labelPima, predictionKNNPimaRS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima KNN with RS:')\n",
    "metrics.precision_score(test_labelPima, predictionKNNPimaRS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos del dataset Wisconsin, entrenamos nuestro clasificador, validamos y obtenemos el accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97857142857142854"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsKNNWiscRS = clfKNNRS.fit(train_attsWisc2, train_labelWisc)\n",
    "predictionKNNWiscRS = clfKNNRS.predict(test_attsWisc2)\n",
    "print('Accuracy Wisconsin KNN with RS:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionKNNWiscRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mejores parámetros obtenidos por el RandomizedSearch para el KNN en este dataset son utilizar 3 vecinos, usando la métrica uniforme para medir las distancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin KNN) con RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RSKNN__n_neighbors': 3, 'RSKNN__weights': 'uniform'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin KNN) con RS:')\n",
    "clsKNNWiscRS.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos la matriz de confusión, precision y recall obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Wisconsin KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[91,  1],\n",
       "       [ 2, 46]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Wisconsin KNN with RS:')\n",
    "metrics.confusion_matrix(test_labelWisc, predictionKNNWiscRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95833333333333337"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin KNN with RS:')\n",
    "metrics.recall_score(test_labelWisc, predictionKNNWiscRS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97872340425531912"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin KNN with RS:')\n",
    "metrics.precision_score(test_labelWisc, predictionKNNWiscRS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, los resultados usando RandomizedSearch y GridSearch son muy similares."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
