{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vanessa Navarro Coronado e Iván Sánchez Castellanos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2 - Clasificación supervisada en scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta práctica vamos a estudiar de nuevo los datasets Pima y Wisconsin. Probaremos los clasificadores de Árbol de Decisión y KNN, y analizaremos cuál es mejor en cada caso. Además, trataremos de descubrir las configuraciones óptimas de dichos algoritmos, y realizaremos un breve estudio sobre ellos.\n",
    "\n",
    "También realizaremos la parte opcional, en la aplicaremos los mismos procedimientos que usamos con el árbol de decisión y el KNN, utilizando un clasificador más. En nuestro caso hemos elegido Naive Bayes.\n",
    "\n",
    "Para comenzar, como siempre, lo primero que hacemos es importar los paquetes necesarios, e inicializar nuestra semilla para nuestros experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always load all scipy stack packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, integrate\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code configures matplotlib for proper rendering\n",
    "%matplotlib inline\n",
    "mpl.rcParams[\"figure.figsize\"] = \"8, 4\"\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=6342\n",
    "np.random.seed(6342)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargado de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, cargamos los datos de los datasets Pima y Wisconsin. Cada uno será separado en dos conjuntos, uno con los atributos y otro con la clase del dataset. Esto es necesario para que la libreria scikit learn pueda ser utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the file path to fit your system\n",
    "dfPima = pd.read_csv(\"../data/pima.csv\", dtype={ \"label\": 'category'})\n",
    "dfAttributesPima = dfPima.drop('label', 1)\n",
    "dfLabelPima = dfPima['label']\n",
    "\n",
    "dfWisc = pd.read_csv(\"../data/wisconsin.csv\", dtype={ \"label\": 'category'})\n",
    "dfAttributesWisc = dfWisc.drop('label', 1)\n",
    "dfLabelWisc = dfWisc['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargados los datasets, debemos dividirlos en dos conjuntos de Train y Test para poder realizar nuestros experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into train/test split for our experiments\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_attsPima, test_attsPima, train_labelPima, test_labelPima = train_test_split( \n",
    "    dfAttributesPima,\n",
    "    dfLabelPima,\n",
    "    test_size=0.2,\n",
    "    random_state=seed,\n",
    "    stratify=dfLabelPima)\n",
    "\n",
    "train_attsWisc, test_attsWisc, train_labelWisc, test_labelWisc = train_test_split( \n",
    "    dfAttributesWisc,\n",
    "    dfLabelWisc,\n",
    "    test_size=0.2,\n",
    "    random_state=seed,\n",
    "    stratify=dfLabelWisc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de los datos en dataset Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder realizar el tratamiento de valores perdidos en el dataset Pima, primero debemos seleccionar los atributos que contengan valores a cero que son considerados como perdidos. En este caso nosotros pensamos que un cero en la variable 'preg' no sería valor perdido, ya que los pacientes puede que no hayan estado embarazados ninguna vez, o que sean de género masculino. Por tanto, un cero en cualquiera de las otras variables predictoras será considerado como valor perdido, a excepción de la variable 'preg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['plas', 'pres', 'skin', 'insu', 'mass', 'pedi', 'age'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_ceros = dfPima.columns.drop([\"preg\",\"label\"])\n",
    "aux_ceros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez separadas las variables sobre las que queremos hacer tratamiento de valores perdidos, reemplazamos los valores a cero por valores de tipo NaN, para que podamos imputar estos valores por la media de las variables para nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in aux_ceros:\n",
    "    train_attsPima.replace({i: {0: np.nan}}, inplace = True)\n",
    "    train_labelPima.replace({i: {0: np.nan}}, inplace = True)\n",
    "    test_attsPima.replace({i: {0: np.nan}}, inplace = True)\n",
    "    test_labelPima.replace({i: {0: np.nan}}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Selección y evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección realizaremos experimentos utilizando el algoritmo GridSearch de la librería scikit-learn para descubrir las configuraciones óptimas de los clasificadores DecisionTree y KNN para los datasets Pima y Wisconsin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pasos a seguir en este apartado son:\n",
    "* Crear un objeto de la clase GridSearchCV, al que le pasaremos:\n",
    "    - el estimador que vamos a utilizar (DecisionTree o KNN), \n",
    "    - los hiperparámetros del clasificador que queremos tener en cuenta a la hora de buscar la configuración óptima.\n",
    "    - También debemos indicar qué tipo de métrica usaremos para valorar las configuraciones,\n",
    "    - cuántos folds se deben realizar en el proceso de validación cruzada que realiza el GridSearchCV. Si en este campo especificamos un objeto de la clase StratifiedKFold con 10 folds y nuestra semilla, podrá realizar un proceso de validación cruzada estratificada.\n",
    "    - iid = False, para que la evaluación de los resultados se haga sobre una media aritmética, y no sobre una media ponderada.\n",
    "* Crear un Pipeline con dos pasos:\n",
    "    - Un objeto de la clase Imputer que se encarga de realizar el tratamiento de valores perdidos. En nuestro caso, imputaremos los valores NaN de los datasets con la media.\n",
    "    - Un clasificador con una configuración óptima de parámetros, la cual ha sido calculada por el GridSearch.\n",
    "* Entrenar y validar el clasificador con los datos de nuestros datasets, y analizar los resultados obtenidos (accuracy, precision y recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Árbol de decisión con preprocesamiento de los datos durante la validación cruzada (transformers y pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos con el clasificador DecisionTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "# Cargamos el arbol de decision\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí definimos nuestro GridSearch con los parámetros elegidos. Como estamos usando el clasificador DecisionTree, podemos pasarle como parámetro de 'random_state' nuestra semilla para poder hacer reproducibles nuestros experimentos. \n",
    "\n",
    "En nuestro caso hemos elegido los parámetros 'criterion' (puede ser \"gini\", que usa la impureza Gini como función, o “entropy”, usando la ganancia de información), 'max_depth' (para indicar la máxima profundidad del árbol), y 'min_samples_leaf' (para indicar el número mínimo de ejemplos que queremos que haya en cada hoja del árbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfTree = GridSearchCV(\n",
    "    estimator = tree.DecisionTreeClassifier(random_state=seed),\n",
    "    param_grid =\n",
    "        {'criterion': [\"entropy\",\"gini\"],'max_depth': [3, 5, 10, None], 'min_samples_leaf': [3,5,10]},\n",
    "    scoring = 'accuracy',\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed), iid=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si asignamos el valor 'None' al parámetro 'max_depth', el clasificador intentará llegar a la máxima profundidad del árbol por defecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación creamos un Pipeline con dos pasos: el primero es imputar los valores perdidos siguiendo la estrategia de rellenar con la media, y el segundo paso es el GridSearch que configuramos en la celda anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the pipeline as a set of tuples\n",
    "estimatorTree = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"GridSearchTree\", clfTree)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de crear el Pipeline, lo ejecutamos con el dataset Pima para entrenar el clasificador con el conjunto de Train, validamos con nuestro conjunto de Test, y después imprimimos el accuracy obtenido con la mejor configuración de parámetros del árbol encontrada por nuestro GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72727272727272729"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreePima = estimatorTree.fit(train_attsPima, train_labelPima)\n",
    "predictionTreePima = estimatorTree.predict(test_attsPima)\n",
    "print('Accuracy Pima Tree:')\n",
    "metrics.accuracy_score(test_labelPima, predictionTreePima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra posible forma de evaluar el Pipeline generado, utilizando la función cross_val_score de la validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70681832603591588"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or we can use it as the input of a cross-val scorer\n",
    "cross_val_score(estimatorTree, train_attsPima, train_labelPima).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, nos parece interesante mostrar la mejor configuración de los parámetros del árbol que ha encontrado el GridSearch. En este caso, la mejor configuración es utilizar el criterio de entropía para evaluar las variables del árbol, emplear una profundidad máxima del árbol de 10, y un número mínimo de ejemplos por hoja igual a 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima Tree):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima Tree):')\n",
    "clsTreePima.named_steps['GridSearchTree'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además del accuracy, también es interesante mostrar la matriz de confusión obtenida, y las medidas de precision y recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Pima Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[79, 21],\n",
       "       [21, 33]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Pima Tree:')\n",
    "metrics.confusion_matrix(test_labelPima, predictionTreePima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61111111111111116"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima Tree:')\n",
    "metrics.recall_score(test_labelPima, predictionTreePima, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61111111111111116"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima Tree:')\n",
    "metrics.precision_score(test_labelPima, predictionTreePima, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el mismo procedimiento con el dataset Wisconsin. Usando el Pipeline creado en el apartado 1.1.2., entrenamos, validamos y obtenemos las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94285714285714284"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreeWisc = estimatorTree.fit(train_attsWisc, train_labelWisc)\n",
    "predictionTreeWisc = estimatorTree.predict(test_attsWisc)\n",
    "print('Accuracy Wisconsin Tree:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionTreeWisc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93384509228911516"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or we can use it as the input of a cross-val scorer\n",
    "cross_val_score(estimatorTree, train_attsWisc, train_labelWisc).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora mostramos la mejor configuración obtenida con el GridSearch, que en este caso consiste en usar el criterio gini para el árbol, una profundidad máxima de 5, y un número mínimo de ejemplos por hoja igual a 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin Tree):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 3}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin Tree):')\n",
    "clsTreeWisc.named_steps['GridSearchTree'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos la matriz de confusión, precision y recall, igual que hicimos con el dataset Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Wisconsin Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[90,  2],\n",
       "       [ 6, 42]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Wisconsin Tree:')\n",
    "metrics.confusion_matrix(test_labelWisc, predictionTreeWisc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin Tree:')\n",
    "metrics.recall_score(test_labelWisc, predictionTreeWisc, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin Tree:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95454545454545459"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin Tree:')\n",
    "metrics.precision_score(test_labelWisc, predictionTreeWisc, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. KNN con preprocesamiento de los datos durante la validación cruzada (transformers y pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte vamos a usar un KNN como clasificador para nuestro Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos los parámetros del algoritmo de KNN que queremos que el GridSearch tenga en cuenta a la hora de buscar la configuración óptima. En el algoritmo KNeighborsClassifier no se nos permite elegir un random_state como en el caso del árbol para poder hacer reproducibles nuestros experimentos, por lo que solo usaremos la semilla en el proceso de validación cruzada que se realiza en el GridSearch.\n",
    "\n",
    "En nuestro caso, para el KNN hemos añadido el número de vecinos, y el tipo de métrica a utilizar para valorar las distancias de los vecinos (puede ser la distancia o la inversa de la distancia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfKNN = GridSearchCV(\n",
    "    estimator = neighbors.KNeighborsClassifier(),\n",
    "    param_grid = \n",
    "        { 'n_neighbors' : [1,2,3,4,5], 'weights': ['uniform','distance'] },\n",
    "    scoring = 'accuracy',\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed), iid=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el Pipeline con el imputer como primer paso, y como segundo paso, el clasificador con la mejor configuración de parámetros obtenida a través del GridSearch especificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the pipeline as a set of tuples\n",
    "estimatorKNN = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"GridSearchKNN\", clfKNN)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero entrenamos, validamos y obtenemos el accuracy en los datos del dataset Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68831168831168832"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsKNNPima = estimatorKNN.fit(train_attsPima, train_labelPima)\n",
    "predictionKNNPima = estimatorKNN.predict(test_attsPima)\n",
    "print('Accuracy Pima KNN:')\n",
    "metrics.accuracy_score(test_labelPima, predictionKNNPima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la métrica de la función cross_val_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75899486007995431"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or we can use it as the input of a cross-val scorer\n",
    "cross_val_score(estimatorKNN, train_attsPima, train_labelPima).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos cuál ha sido la mejor configuración de parámetros obtenida por el GridSearch. En este caso es utilizar 5 vecinos, usando la métrica uniforme para medir las distancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima KNN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 5, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima KNN):')\n",
    "clsKNNPima.named_steps['GridSearchKNN'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación mostramos la matriz de confusión, precision y recall obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Pima KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[82, 18],\n",
       "       [30, 24]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Pima KNN:')\n",
    "metrics.confusion_matrix(test_labelPima, predictionKNNPima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44444444444444442"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima KNN:')\n",
    "metrics.recall_score(test_labelPima, predictionKNNPima, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima KNN:')\n",
    "metrics.precision_score(test_labelPima, predictionKNNPima, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos del dataset Wisconsin, entrenamos nuestro clasificador, validamos y obtenemos el accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsKNNWisc = estimatorKNN.fit(train_attsWisc, train_labelWisc)\n",
    "predictionKNNWisc = estimatorKNN.predict(test_attsWisc)\n",
    "print('Accuracy Wisconsin KNN:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionKNNWisc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medida del cross_val_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93741015467770694"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or we can use it as the input of a cross-val scorer\n",
    "cross_val_score(estimatorKNN, train_attsWisc, train_labelWisc).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mejores parámetros obtenidos por el GridSearch para el KNN en este dataset son utilizar 1 solo vecino, usando la métrica uniforme para medir las distancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin KNN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin KNN):')\n",
    "clsKNNWisc.named_steps['GridSearchKNN'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos la matriz de confusión, precision y recall obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Wisconsin KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[91,  1],\n",
       "       [ 6, 42]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Wisconsin KNN:')\n",
    "metrics.confusion_matrix(test_labelWisc, predictionKNNWisc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin KNN:')\n",
    "metrics.recall_score(test_labelWisc, predictionKNNWisc, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin KNN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97674418604651159"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin KNN:')\n",
    "metrics.precision_score(test_labelWisc, predictionKNNWisc, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Comparativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado intentaremos analizar cuál de los clasificadores ha obtenido mejores resultados en los datasets estudiados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataset Pima los resultados de accuracy para el árbol de decisión y el KNN con k=5, son: \n",
    "* Accuracy Pima Tree: 0.72727272727272729\n",
    "* Accuracy Pima 5-NN: 0.68831168831168832\n",
    "\n",
    "En cuanto a los resultados de recall tenemos:\n",
    "* Recall Pima Tree: 0.61111111111111116\n",
    "* Recall Pima 5-NN: 0.44444444444444442\n",
    "\n",
    "Y los de precision:\n",
    "* Precision Pima Tree: 0.61111111111111116\n",
    "* Precision Pima 5-NN: 0.5714285714285714\n",
    "\n",
    "Por tanto, podemos concluir que el mejor modelo es el obtenido con el árbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataset Wisconsin, los resultados de acurracy para el árbol de decisión y el KNN con k=1 son:\n",
    "* Accuracy Wisconsin Tree: 0.94285714285714284\n",
    "* Accuracy Wisconsin 1-NN: 0.94999999999999996\n",
    "\n",
    "Los resultados de recall son:\n",
    "* Recall Wisconsin Tree: 0.875\n",
    "* Recall Wisconsin 1-NN: 0.875\n",
    "\n",
    "Y los de precision:\n",
    "* Precision Wisconsin Tree: 0.95454545454545459\n",
    "* Precision Wisconsin 1-NN: 0.97674418604651159\n",
    "\n",
    "Como vemos, el accuracy y el recall son bastante parecidos entre ellos, pero gracias al resultado de precision podemos concluir que el mejor modelo podría ser el KNN en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Estudio de los algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado vamos a estudiar invidualmente los algoritmos:\n",
    "* En KNN, estudiaremos los parámetros aprendidos por el clasificador, realizando varias pruebas con distintas configuraciones de parámetros y analizando los resultados. Analizaremos por separado los dos datasets.\n",
    "* Para el árbol, estudiaremos los parámetros aprendidos por el clasificador, y también su estructura. Analizaremos las estructuras del clasificador con la configuración óptima y otra con una configuración suboptima, y valoraremos los resultados.\n",
    "\n",
    "Para analizarlos, primero vamos a crear una funcion getScores que será similar a nuestra implementación del GridSearch pero más sencilla. Simplemente obtendremos los scores usando los distintos parámetros del clasificador. No realizamos proceso de validación cruzada, solamente usamos el holdout que teníamos al principio de la práctica.\n",
    "\n",
    "Después obtenemos tiempo y accuracy para las configuraciones, y realizaremos una comparativa de todos ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from time import time\n",
    "\n",
    "def getScores(estim,paramG,train_atts,train_label,test_atts,test_label):\n",
    "    \n",
    "    scores=[] #para almacenar los resultados de accuracy a devolver\n",
    "    exTimes=[] #para almacenar los tiempos a devolver\n",
    "    \n",
    "    #Generamos las configuraciones de parametros como en GridS\n",
    "    items = paramG.items()\n",
    "    keys, values = zip(*items)\n",
    "    v = list(product(*values))\n",
    "    \n",
    "    # Recorremos las configuraciones y almacenamos los resultados\n",
    "    for param in v:\n",
    "        \n",
    "        params = dict(zip(keys,param))\n",
    "        estim.set_params(**params)\n",
    "        \n",
    "        timeSum=0 #variable intermedia para mostrar la media de los tiempos\n",
    "        \n",
    "        for i in range(0,100):\n",
    "            start = time()\n",
    "            estim.fit(train_atts,train_label) #entrenamos con el conjunto de Train\n",
    "            predictions = estim.predict(test_atts) #array de predicciones para el conjunto de Test\n",
    "            end = time()\n",
    "            timeSum += (end - start)\n",
    "        exTimes.append(timeSum/100) #añadimos al array de tiempos la media de las 100 medidas de tiempo realizadas\n",
    "        \n",
    "        comparison = np.sum(predictions == test_label) #comparamos predicciones con test_label y sumamos los aciertos\n",
    "        accuracy = comparison / len(predictions) #dividimos entre los casos totales\n",
    "        scores.append(accuracy) #añadimos al array de scores el accuracy obtenido con la configuración de parámetros actual\n",
    "        \n",
    "    return (v,scores,exTimes) #devolvemos las configuraciones, los accuracys y los tiempos de cada configuracion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Estudio del algoritmo KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este clasificador, usaremos las combinaciones teniendo en cuenta el número de vecinos (k=1,5,10,50,100) y el método de evaluación de las distancias (uniforme o inversa de la distancia). Para ello usaremos nuestra función getScores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.1. KNN con Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero de todo es imputar con la media los valores perdidos del dataset Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp=Imputer(missing_values='NaN', strategy=\"mean\",axis=0)\n",
    "#we fit it\n",
    "impPima = imp.fit(train_attsPima)\n",
    "# Finally we can use it to transform any dataframe:\n",
    "train_attsPima_clean = impPima.transform(train_attsPima)\n",
    "test_attsPima_clean = impPima.transform(test_attsPima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecho esto, podemos hacer una comparativa de los resultados obtenidos para todas las configuraciones del clasificador KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paramsKNNPima</th>\n",
       "      <th>scoresKNNPima</th>\n",
       "      <th>exTimesKNNPima</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(uniform, 1)</td>\n",
       "      <td>0.629870</td>\n",
       "      <td>0.002171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(uniform, 5)</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.002180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(uniform, 10)</td>\n",
       "      <td>0.694805</td>\n",
       "      <td>0.002285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(uniform, 50)</td>\n",
       "      <td>0.733766</td>\n",
       "      <td>0.003453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(uniform, 100)</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.005001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(distance, 1)</td>\n",
       "      <td>0.629870</td>\n",
       "      <td>0.001899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(distance, 5)</td>\n",
       "      <td>0.694805</td>\n",
       "      <td>0.002125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(distance, 10)</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.002364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(distance, 50)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.003605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(distance, 100)</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.005108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paramsKNNPima  scoresKNNPima  exTimesKNNPima\n",
       "0     (uniform, 1)       0.629870        0.002171\n",
       "1     (uniform, 5)       0.688312        0.002180\n",
       "2    (uniform, 10)       0.694805        0.002285\n",
       "3    (uniform, 50)       0.733766        0.003453\n",
       "4   (uniform, 100)       0.707792        0.005001\n",
       "5    (distance, 1)       0.629870        0.001899\n",
       "6    (distance, 5)       0.694805        0.002125\n",
       "7   (distance, 10)       0.701299        0.002364\n",
       "8   (distance, 50)       0.746753        0.003605\n",
       "9  (distance, 100)       0.727273        0.005108"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsKNNPima,scoresKNNPima,exTimesKNNPima=getScores(neighbors.KNeighborsClassifier(),\n",
    "                                                     { 'weights': ['uniform','distance'], 'n_neighbors' : [1,5,10,50,100] },\n",
    "                                                     train_attsPima_clean,train_labelPima,\n",
    "                                                     test_attsPima_clean,test_labelPima)\n",
    "resultadosKNNPima=pd.DataFrame(list(zip(paramsKNNPima,scoresKNNPima,exTimesKNNPima)))\n",
    "resultadosKNNPima.columns=['paramsKNNPima','scoresKNNPima','exTimesKNNPima']\n",
    "resultadosKNNPima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos comprobar, la configuración con mejor accuracy sería usar k=50 y la inversa de la distancia (fila 8 de la tabla). Por tanto, si aumentamos K, podríamos pensar que se mejora el accuracy, pero solo hasta cierto punto, porque con K=100 (fila 9) ya obtenemos un accuracy peor. De forma general, usar un K muy alto se asemeja a un clasificador ZeroR, ya que ambos utilizarían la estrategia de clasificación por la clase mayoritaria.\n",
    "\n",
    "Además, debemos tener en mente que cuanto mayor sea el parámetro K, mayor será el tiempo de ejecución. En este caso, el tiempo de ejecución se corresponde con el tiempo de realizar la predicción con el conjunto de Test. Esto es debido a que el entrenamiento en el clasificador KNN es muy rápido, ya que solo consiste en copiar la base de datos de los casos de Train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.2. KNN con Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estudiaremos las configuraciones obtenidas con el dataset Wisconsin. Primero debemos imputar los valores perdidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we fit it\n",
    "impWisc = imp.fit(train_attsWisc)\n",
    "# Finally we can use it to transform any dataframe:\n",
    "train_attsWisc_clean = impWisc.transform(train_attsWisc)\n",
    "test_attsWisc_clean = impWisc.transform(test_attsWisc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecho esto, realizamos la comparativa de los resultados obtenidos para todas las configuraciones del clasificador KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paramsKNNWisc</th>\n",
       "      <th>scoresKNNWisc</th>\n",
       "      <th>exTimesKNNWisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(uniform, 1)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(uniform, 5)</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.001661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(uniform, 10)</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.001720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(uniform, 50)</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.002420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(uniform, 100)</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.003278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(distance, 1)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(distance, 5)</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.001653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(distance, 10)</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.001720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(distance, 50)</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.002515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(distance, 100)</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.003709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paramsKNNWisc  scoresKNNWisc  exTimesKNNWisc\n",
       "0     (uniform, 1)       0.950000        0.001999\n",
       "1     (uniform, 5)       0.928571        0.001661\n",
       "2    (uniform, 10)       0.850000        0.001720\n",
       "3    (uniform, 50)       0.657143        0.002420\n",
       "4   (uniform, 100)       0.671429        0.003278\n",
       "5    (distance, 1)       0.950000        0.001712\n",
       "6    (distance, 5)       0.942857        0.001653\n",
       "7   (distance, 10)       0.892857        0.001720\n",
       "8   (distance, 50)       0.742857        0.002515\n",
       "9  (distance, 100)       0.742857        0.003709"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsKNNWisc,scoresKNNWisc,exTimesKNNWisc=getScores(neighbors.KNeighborsClassifier(),\n",
    "                                                     { 'weights': ['uniform','distance'], 'n_neighbors' : [1,5,10,50,100] },\n",
    "                                                     train_attsWisc_clean,train_labelWisc,\n",
    "                                                     test_attsWisc_clean,test_labelWisc)\n",
    "resultadosKNNWisc=pd.DataFrame(list(zip(paramsKNNWisc,scoresKNNWisc,exTimesKNNWisc)))\n",
    "resultadosKNNWisc.columns=['paramsKNNWisc','scoresKNNWisc','exTimesKNNWisc']\n",
    "resultadosKNNWisc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos comprobar, la configuración con mejor accuracy sería usar k=1 y la inversa de la distancia (fila 5 de la tabla). \n",
    "\n",
    "Con este dataset, a diferencia de lo que observamos con Pima, vemos que la mejor opción es usar valores de K muy pequeños, ya que los accuracys empeoran bastante con K's más grandes. Además, cuanto mayor es K, mayor será el tiempo de ejecución del algoritmo. \n",
    "\n",
    "Como ya mencionamos anteriormente, de forma general podemos decir que usar un K muy alto en KNN se asemeja a un clasificador ZeroR, ya que ambos utilizarían la estrategia de clasificación por la clase mayoritaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2. Estudio del algoritmo DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este clasificador, realizaremos las combinaciones teniendo en cuenta el criterio (entropía o gini), la profundidad máxima del árbol, y el número mínimo de ejemplos por hoja. \n",
    "\n",
    "Para analizar la estructura de los árboles generados con las diferentes combinaciones de parámetros, vamos a usar (además de nuestra función getScores) la función getTreeNodes, que nos devuelve el número de nodos que presenta un árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTreeNodes(estim,paramG,train_atts,train_label):\n",
    "    \n",
    "    nodes=[] #para almacenar el numero de nodos de cada arbol generado\n",
    "    \n",
    "    #Generamos las configuraciones de parametros como en GridS\n",
    "    items = paramG.items()\n",
    "    keys, values = zip(*items)\n",
    "    v = list(product(*values))\n",
    "    \n",
    "    # Recorremos las configuraciones y almacenamos los resultados\n",
    "    for param in v:\n",
    "        \n",
    "        params = dict(zip(keys,param))\n",
    "        estim.set_params(**params)\n",
    "\n",
    "        estim.fit(train_atts,train_label) #entrenamos con el conjunto de Train\n",
    "        \n",
    "        nodes.append(estim.tree_.node_count) #añadimos al array el numero de nodos del arbol actual\n",
    "        \n",
    "    return (v,nodes) #devolvemos las configuraciones de los arboles y el numero de nodos de cada uno de ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.1. Árbol con Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzaremos con el dataset Pima. Vamos a hacer una comparativa de los resultados obtenidos para todas las configuraciones del árbol de clasificación, junto con la complejidad de cada árbol generado (representada por el número de nodos de cada árbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsTreePima,scoresTreePima,exTimesTreePima=getScores(tree.DecisionTreeClassifier(random_state=seed),\n",
    "                                                        {'criterion': [\"entropy\",\"gini\"],\n",
    "                                                         'max_depth': [3, 5, 10], \n",
    "                                                         'min_samples_leaf': [3,5,10]},\n",
    "                                                        train_attsPima_clean,train_labelPima,\n",
    "                                                        test_attsPima_clean,test_labelPima)\n",
    "resultadosTreePima=pd.DataFrame(list(zip(paramsTreePima,scoresTreePima,exTimesTreePima)))\n",
    "resultadosTreePima.columns=['paramsTreePima','scoresTreePima','exTimesTreePima']\n",
    "#resultadosTreePima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paramsTreePima</th>\n",
       "      <th>scoresTreePima</th>\n",
       "      <th>exTimesTreePima</th>\n",
       "      <th>nodesTreePima</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(entropy, 3, 3)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(entropy, 3, 5)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(entropy, 3, 10)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(entropy, 5, 3)</td>\n",
       "      <td>0.720779</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(entropy, 5, 5)</td>\n",
       "      <td>0.720779</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(entropy, 5, 10)</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(entropy, 10, 3)</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(entropy, 10, 5)</td>\n",
       "      <td>0.720779</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(entropy, 10, 10)</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(gini, 3, 3)</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(gini, 3, 5)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(gini, 3, 10)</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(gini, 5, 3)</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(gini, 5, 5)</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(gini, 5, 10)</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(gini, 10, 3)</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(gini, 10, 5)</td>\n",
       "      <td>0.759740</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(gini, 10, 10)</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paramsTreePima  scoresTreePima  exTimesTreePima  nodesTreePima\n",
       "0     (entropy, 3, 3)        0.746753         0.002352             15\n",
       "1     (entropy, 3, 5)        0.746753         0.002099             15\n",
       "2    (entropy, 3, 10)        0.746753         0.002005             15\n",
       "3     (entropy, 5, 3)        0.720779         0.003167             39\n",
       "4     (entropy, 5, 5)        0.720779         0.003060             41\n",
       "5    (entropy, 5, 10)        0.707792         0.002790             43\n",
       "6    (entropy, 10, 3)        0.727273         0.004198            131\n",
       "7    (entropy, 10, 5)        0.720779         0.003836            111\n",
       "8   (entropy, 10, 10)        0.727273         0.003182             73\n",
       "9        (gini, 3, 3)        0.753247         0.001541             15\n",
       "10       (gini, 3, 5)        0.746753         0.001482             15\n",
       "11      (gini, 3, 10)        0.746753         0.001442             15\n",
       "12       (gini, 5, 3)        0.714286         0.001874             41\n",
       "13       (gini, 5, 5)        0.707792         0.001875             41\n",
       "14      (gini, 5, 10)        0.714286         0.001832             39\n",
       "15      (gini, 10, 3)        0.740260         0.002572            139\n",
       "16      (gini, 10, 5)        0.759740         0.002368            107\n",
       "17     (gini, 10, 10)        0.707792         0.002116             69"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsTreePima,nodesTreePima=getTreeNodes(tree.DecisionTreeClassifier(random_state=seed),\n",
    "                                          {'criterion': [\"entropy\",\"gini\"],\n",
    "                                           'max_depth': [3, 5, 10], \n",
    "                                           'min_samples_leaf': [3,5,10]},\n",
    "                                          train_attsPima_clean,train_labelPima)\n",
    "resultadosTreePima.assign(nodesTreePima = nodesTreePima)\n",
    "#resultadosTreePima['nodesTreePima']=nodesTreePima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos en la tabla, la configuración con mayor accuracy sería la que se encuentra en la fila 16: criterio gini, máxima profundidad del árbol igual a 10 y mínimo número de ejemplos por hoja igual a 5.\n",
    "\n",
    "Sin embargo, esta configuración genera un árbol muy complejo, con más de 100 nodos. Por tanto, podríamos compararlo con el segundo mejor árbol generado (árbol de la fila 9 de la tabla), el cual presenta un accuracy algo más bajo, pero está compuesto por tan solo 15 nodos. Este caso necesitaría una profundidad máxima del árbol igual a 3, y un número mínimo de ejemplos por hoja igual a 3, para conseguir (casi) igualar el accuracy al del árbol de la fila 16, por lo que sería interesante seleccionar el árbol más simple para nuestra clasificación.\n",
    "\n",
    "De esta forma podemos ver que debemos tener en cuenta otros factores además del accuracy para poder evaluar la efectividad del clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.2. Árbol con Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora haremos lo mismo con los árboles generados para el dataset Wisconsin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsTreeWisc,scoresTreeWisc,exTimesTreeWisc=getScores(tree.DecisionTreeClassifier(random_state=seed),\n",
    "                                                        {'criterion': [\"entropy\",\"gini\"],\n",
    "                                                         'max_depth': [3, 5, 10], \n",
    "                                                         'min_samples_leaf': [3,5,10]},\n",
    "                                                        train_attsWisc_clean,train_labelWisc,\n",
    "                                                        test_attsWisc_clean,test_labelWisc)\n",
    "resultadosTreeWisc=pd.DataFrame(list(zip(paramsTreeWisc,scoresTreeWisc,exTimesTreeWisc)))\n",
    "resultadosTreeWisc.columns=['paramsTreeWisc','scoresTreeWisc','exTimesTreeWisc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paramsTreeWisc</th>\n",
       "      <th>scoresTreeWisc</th>\n",
       "      <th>exTimesTreeWisc</th>\n",
       "      <th>nodesTreeWisc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(entropy, 3, 3)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(entropy, 3, 5)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(entropy, 3, 10)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(entropy, 5, 3)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(entropy, 5, 5)</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(entropy, 5, 10)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(entropy, 10, 3)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(entropy, 10, 5)</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(entropy, 10, 10)</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(gini, 3, 3)</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(gini, 3, 5)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(gini, 3, 10)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(gini, 5, 3)</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(gini, 5, 5)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(gini, 5, 10)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(gini, 10, 3)</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(gini, 10, 5)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(gini, 10, 10)</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paramsTreeWisc  scoresTreeWisc  exTimesTreeWisc  nodesTreeWisc\n",
       "0     (entropy, 3, 3)        0.950000         0.001796             15\n",
       "1     (entropy, 3, 5)        0.950000         0.001355             15\n",
       "2    (entropy, 3, 10)        0.950000         0.001304             15\n",
       "3     (entropy, 5, 3)        0.964286         0.001459             33\n",
       "4     (entropy, 5, 5)        0.971429         0.001429             29\n",
       "5    (entropy, 5, 10)        0.950000         0.001371             25\n",
       "6    (entropy, 10, 3)        0.964286         0.001477             37\n",
       "7    (entropy, 10, 5)        0.971429         0.001521             31\n",
       "8   (entropy, 10, 10)        0.950000         0.001424             25\n",
       "9        (gini, 3, 3)        0.964286         0.001055             15\n",
       "10       (gini, 3, 5)        0.957143         0.001049             15\n",
       "11      (gini, 3, 10)        0.957143         0.001033             13\n",
       "12       (gini, 5, 3)        0.942857         0.001252             33\n",
       "13       (gini, 5, 5)        0.957143         0.001244             29\n",
       "14      (gini, 5, 10)        0.957143         0.001230             23\n",
       "15      (gini, 10, 3)        0.935714         0.001269             41\n",
       "16      (gini, 10, 5)        0.957143         0.001245             31\n",
       "17     (gini, 10, 10)        0.957143         0.001257             23"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsTreeWisc,nodesTreeWisc=getTreeNodes(tree.DecisionTreeClassifier(random_state=seed),\n",
    "                                          {'criterion': [\"entropy\",\"gini\"],\n",
    "                                           'max_depth': [3, 5, 10], \n",
    "                                           'min_samples_leaf': [3,5,10]},\n",
    "                                          train_attsWisc_clean,train_labelWisc)\n",
    "resultadosTreeWisc.assign(nodesTreeWisc = nodesTreeWisc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos en la tabla, la configuración con mayor accuracy sería la que se encuentra en la fila 7: criterio entropía, máxima profundidad del árbol igual a 10 y mínimo número de ejemplos por hoja igual a 5.\n",
    "\n",
    "En este caso, los árboles no son demasiado complejos, todos tienen un número de nodos similar. Aún así podemos observar algunas pequeñas diferencias en dicho número.\n",
    "\n",
    "Una configuración subóptima que sería interesante analizar es la que se encuentra en la fila 9 (criterio gini, máxima profundidad de 3 y mínimo número de ejemplos por hoja igual a 3), ya que obtiene un accuracy parecido al árbol de la fila 7, pero presenta un número de nodos menor (se reduce a la mitad). Por ello, podría resultar interesante seleccionar el árbol más simple para nuestra clasificación.\n",
    "\n",
    "De esta forma, al igual que en el dataset Pima, llegamos a la conclusión de que debemos tener en cuenta otros factores además del accuracy para poder evaluar la efectividad del clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3. Conclusiones del estudio de los algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras el estudio de los algoritmos, nos hemos dado cuenta de que dependiendo del problema y del dataset al que nos enfrentemos, es posible que la configuración de parámetros óptima para un determinado algoritmo no siempre sea la misma. \n",
    "* En el KNN con el dataset Pima hemos visto que es bueno aumentar la vecindad (hasta cierto punto). Sin embargo, con el dataset Wisconsin los resultados empeoran mucho al usar vecindades grandes, y además se consumía demasiado tiempo.\n",
    "* Con el árbol de decisión, hemos aprendido que es una buena práctica observar las configuraciones subóptimas del clasificador para obtener árboles más sencillos, en los cuales el tiempo de ejecución es menor y obtienen rendimientos similares a árboles más grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementación de GridSearch manualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado realizamos una implementación manual de un GridSearh básico, con su constructor, y las funciones fit y predict, que son las que utilizamos en esta práctica.\n",
    "\n",
    "Nuestra clase GridS tiene como atributos:\n",
    "* estim: estimador o clasificador a evaluar,\n",
    "* crossval: cómo realizar el proceso de validación cruzada durante la evaluación,\n",
    "* score: tipo de métrica a utilizar para la evaluación de la configuración óptima,\n",
    "* keys: nombre de los parámetros del clasificador a tener en cuenta en la evaluación,\n",
    "* v: lista con todas las posibles combinaciones de parámetros del clasificador,\n",
    "* bestScore: mejor puntuación obtenida con la configuración de parámetros del clasificador,\n",
    "* bestParams: mejor configuración de parámetros del clasificador.\n",
    "\n",
    "La función fit se encarga de realizar la validación cruzada para cada una de las diferentes combinaciones de parámetros contenidas en self.v, obteniendo así aquella configuración que ofrezca una mayor puntuación. Una vez obtenida, entrena el clasificador con el conjunto de datos que se le pasa como parámetro a la función fit.\n",
    "\n",
    "La función predict simplemente hace la validación del clasificador con el conjunto que reciba como parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "\n",
    "class GridS(object):\n",
    "    def __init__(self,estim, paramG, score, crossval):\n",
    "        items = paramG.items()\n",
    "        self.keys, values = zip(*items)\n",
    "        self.v = list(product(*values))\n",
    "        self.estim=estim\n",
    "        self.crossval=crossval\n",
    "        self.score=score\n",
    "            \n",
    "    def fit(self,atts,label):\n",
    "        self.bestScore=0\n",
    "        for param in self.v:\n",
    "            params = dict(zip(self.keys,param))\n",
    "            self.estim.set_params(**params)\n",
    "            scores = cross_val_score(estimator=self.estim, \n",
    "                                     X=atts, y=label, \n",
    "                                     scoring=self.score, \n",
    "                                     cv=self.crossval)\n",
    "            if self.bestScore < scores.mean():\n",
    "                self.bestScore=scores.mean()\n",
    "                self.bestParams=params\n",
    "        self.estim.set_params(**self.bestParams)\n",
    "        self.estim.fit(atts,label)\n",
    "    \n",
    "    def predict(self, atts):\n",
    "        return self.estim.predict(atts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. KNN con GridSearch manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que hicimos anteriormente con el GridSearch de scikit, creamos un objeto de la clase GridS. Le pasamos un clasificador KNN, y le indicamos que obtenga la mejor configuración de parámetros atendiendo al número de vecinos (de 1 a 5 vecinos), y el tipo de métrica a utilizar para valorar la distancia (uniforme o inversa de la distancia). El tipo de métrica a utilizar será de nuevo el accuracy, y utilizaremos un proceso de validación cruzada estratificada de 10 folds, con nuestra semilla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsKNN = GridS(\n",
    "    estim = neighbors.KNeighborsClassifier(),\n",
    "    paramG = \n",
    "        { 'n_neighbors' : [1,2,3,4,5], 'weights': ['uniform','distance'] },\n",
    "    score = 'accuracy',\n",
    "    crossval = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después creamos un Pipeline que tiene como primer paso el Imputer de los datos, y como segundo paso el clasificador con la configuración de parámetros obtenida por nuestro GridS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorGSKNN = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"GridSKNN\", gsKNN)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación utilizamos nuestro GridS en el dataset Pima y evaluamos los resultados. Como vemos, son los mismos que obtuvimos usando el GridSearch de scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68831168831168832"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clsKNNPimaGS = estimatorGSKNN.fit(train_attsPima, train_labelPima)\n",
    "predictionKNNPimaGS = estimatorGSKNN.predict(test_attsPima)\n",
    "print('Accuracy Pima KNN + GridS:')\n",
    "metrics.accuracy_score(test_labelPima, predictionKNNPimaGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima KNN + GridS):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 5, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima KNN + GridS):')\n",
    "clsKNNPimaGS.named_steps['GridSKNN'].bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima KNN + GridS:')\n",
    "metrics.precision_score(test_labelPima, predictionKNNPimaGS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44444444444444442"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima KNN + GridS:')\n",
    "metrics.recall_score(test_labelPima, predictionKNNPimaGS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos lo mismo con el dataset Wisconsin. Como vemos, se obtienen los mismos resultados que teníamos usando el GridSearch de scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clsKNNWiscGS = estimatorGSKNN.fit(train_attsWisc, train_labelWisc)\n",
    "predictionKNNWiscGS = estimatorGSKNN.predict(test_attsWisc)\n",
    "print('Accuracy Wisconsin KNN + GridS:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionKNNWiscGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin KNN + GridS):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin KNN + GridS):')\n",
    "clsKNNWiscGS.named_steps['GridSKNN'].bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97674418604651159"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin KNN + GridS:')\n",
    "metrics.precision_score(test_labelWisc, predictionKNNWiscGS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin KNN + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin KNN + GridS:')\n",
    "metrics.recall_score(test_labelWisc, predictionKNNWiscGS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Árbol de decisión con GridSearch manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que hicimos anteriormente con el GridSearch de scikit, creamos un objeto de la clase GridS. Le pasamos un clasificador DecisionTree, y le indicamos que obtenga la mejor configuración de parámetros atendiendo al criterio de evaluación (entrioía o gini), la máxima profundidad del árbol, y el número mínimo de ejemplos por hoja. El tipo de métrica a utilizar será de nuevo el accuracy, y utilizaremos un proceso de validación cruzada estratificada de 10 folds, con nuestra semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsTree=GridS(\n",
    "    estim = tree.DecisionTreeClassifier(random_state=seed),\n",
    "    paramG={'criterion': [\"entropy\",\"gini\"],'max_depth': [3, 5, 10, None], 'min_samples_leaf': [3,5,10]},\n",
    "    score='accuracy',\n",
    "    crossval=StratifiedKFold(n_splits=10, shuffle=False, random_state=seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorGSTree = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"GridSTree\", gsTree)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación utilizamos nuestro GridS en el dataset Pima y evaluamos los resultados. Como vemos, son los mismos que obtuvimos usando el GridSearch de scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72727272727272729"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreePimaGS = estimatorGSTree.fit(train_attsPima, train_labelPima)\n",
    "predictionTreePimaGS = estimatorGSTree.predict(test_attsPima)\n",
    "print('Accuracy Pima Tree + GridS:')\n",
    "metrics.accuracy_score(test_labelPima, predictionTreePimaGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima Tree + GridS):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima Tree + GridS):')\n",
    "clsTreePimaGS.named_steps['GridSTree'].bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61111111111111116"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima Tree + GridS:')\n",
    "metrics.precision_score(test_labelPima, predictionTreePimaGS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61111111111111116"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima Tree + GridS:')\n",
    "metrics.recall_score(test_labelPima, predictionTreePimaGS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos lo mismo con el dataset Wisconsin. Como vemos, se obtienen los mismos resultados que teníamos usando el GridSearch de scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94285714285714284"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreeWiscGS = estimatorGSTree.fit(train_attsWisc, train_labelWisc)\n",
    "predictionTreeWiscGS = estimatorGSTree.predict(test_attsWisc)\n",
    "print('Accuracy Wisconsin Tree + GridS:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionTreeWiscGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin Tree + GridS):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 3}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin Tree + GridS):')\n",
    "clsTreeWiscGS.named_steps['GridSTree'].bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95454545454545459"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin Tree + GridS:')\n",
    "metrics.precision_score(test_labelWisc, predictionTreeWiscGS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin Tree + GridS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin Tree + GridS:')\n",
    "metrics.recall_score(test_labelWisc, predictionTreeWiscGS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Estudio de un kernel de kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extras: Algoritmo RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección realizaremos experimentos utilizando el algoritmo RandomizedSearch de la librería scikit-learn para descubrir las configuraciones óptimas de los clasificadores DecisionTree y KNN para los datasets Pima y Wisconsin. Seguiremos el mismo proceso que hicimos con GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Árbol de decisión con preprocesamiento de los datos durante la validación cruzada (transformers y pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. RandomizedSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí definimos nuestro RandomizedSearch con los parámetros elegidos. Como estamos usando el clasificador DecisionTree, podemos pasarle como parámetro de 'random_state' nuestra semilla para poder hacer reproducibles nuestros experimentos. \n",
    "\n",
    "En nuestro caso hemos elegido los parámetros 'criterion' (puede ser \"gini\", que usa la impureza Gini como función, o “entropy”, usando la ganancia de información), 'max_depth' (para indicar la máxima profundidad del árbol), y 'min_samples_leaf' (para indicar el número mínimo de ejemplos que queremos que haya en cada hoja del árbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfTreeRS = RandomizedSearchCV(\n",
    "    estimator = tree.DecisionTreeClassifier(random_state=seed),\n",
    "    param_distributions =\n",
    "        {'criterion': [\"entropy\",\"gini\"],'max_depth': [3, 5, 10, None], 'min_samples_leaf': [3,5,10]},\n",
    "    scoring = 'accuracy',\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed), \n",
    "    iid=False,\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación creamos un Pipeline con dos pasos: el primero es imputar los valores perdidos siguiendo la estrategia de rellenar con la media, y el segundo paso es el RandomizedSearch que configuramos en la celda anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the pipeline as a set of tuples\n",
    "estimatorTreeRS = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"RandomizedSearchTree\", clfTreeRS)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de crear el Pipeline, lo ejecutamos con el dataset Pima para entrenar el clasificador con el conjunto de Train, validamos con nuestro conjunto de Test, y después imprimimos el accuracy obtenido con la mejor configuración de parámetros del árbol encontrada por nuestro RandomizedSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72727272727272729"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreePimaRS = estimatorTreeRS.fit(train_attsPima, train_labelPima)\n",
    "predictionTreePimaRS = estimatorTreeRS.predict(test_attsPima)\n",
    "print('Accuracy Pima Tree with RS:')\n",
    "metrics.accuracy_score(test_labelPima, predictionTreePima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, nos parece interesante mostrar la mejor configuración de los parámetros del árbol que ha encontrado el GridSearch. En este caso, la mejor configuración es utilizar el criterio de entropía para evaluar las variables del árbol, emplear una profundidad máxima del árbol de 3, y un número mínimo de ejemplos por hoja igual a 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima Tree) con RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 10}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima Tree) con RS:')\n",
    "clsTreePimaRS.named_steps['RandomizedSearchTree'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además del accuracy, también es interesante mostrar la matriz de confusión obtenida, y las medidas de precision y recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Pima Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85, 15],\n",
       "       [24, 30]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Pima Tree with RS:')\n",
    "metrics.confusion_matrix(test_labelPima, predictionTreePimaRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55555555555555558"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima Tree with RS:')\n",
    "metrics.recall_score(test_labelPima, predictionTreePimaRS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66666666666666663"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima Tree with RS:')\n",
    "metrics.precision_score(test_labelPima, predictionTreePimaRS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos el mismo procedimiento con el dataset Wisconsin. Usando el Pipeline creado en el apartado 4.1.2., entrenamos, validamos y obtenemos las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94285714285714284"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsTreeWiscRS = estimatorTreeRS.fit(train_attsWisc, train_labelWisc)\n",
    "predictionTreeWiscRS = estimatorTreeRS.predict(test_attsWisc)\n",
    "print('Accuracy Wisconsin Tree with RS:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionTreeWiscRS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora mostramos la mejor configuración obtenida con el RandomizedSearch, que en este caso consiste en usar el criterio gini para el árbol, una profundidad máxima de 5, y un número mínimo de ejemplos por hoja igual a 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin Tree) con RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 3}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin Tree) con RS:')\n",
    "clsTreeWiscRS.named_steps['RandomizedSearchTree'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos la matriz de confusión, precision y recall, igual que hicimos con el dataset Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Wisconsin Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[90,  2],\n",
       "       [ 6, 42]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Wisconsin Tree with RS:')\n",
    "metrics.confusion_matrix(test_labelWisc, predictionTreeWiscRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin Tree with RS:')\n",
    "metrics.recall_score(test_labelWisc, predictionTreeWiscRS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin Tree with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95454545454545459"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin Tree with RS:')\n",
    "metrics.precision_score(test_labelWisc, predictionTreeWiscRS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. KNN con preprocesamiento de los datos durante la validación cruzada (transformers y pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte vamos a usar un KNN como clasificador para nuestro Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. RandomizedSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos los parámetros del algoritmo de KNN que queremos que el RandomizedSearch tenga en cuenta a la hora de buscar la configuración óptima. En el algoritmo KNeighborsClassifier no se nos permite elegir un random_state como en el caso del árbol para poder hacer reproducibles nuestros experimentos, por lo que solo usaremos la semilla en el proceso de validación cruzada que se realiza en el RandomizedSearch.\n",
    "\n",
    "En nuestro caso, para el KNN hemos añadido el número de vecinos, y el tipo de métrica a utilizar para valorar las distancias de los vecinos (puede ser la distancia o la inversa de la distancia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfKNNRS = RandomizedSearchCV(\n",
    "    estimator = neighbors.KNeighborsClassifier(),\n",
    "    param_distributions = \n",
    "        { 'n_neighbors' : [1,2,3,4,5], 'weights': ['uniform','distance'] },\n",
    "    scoring = 'accuracy',\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed), \n",
    "    iid=False,\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el Pipeline con el imputer como primer paso, y como segundo paso, el clasificador con la mejor configuración de parámetros obtenida a través del RandomizedSearch especificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the pipeline as a set of tuples\n",
    "estimatorKNNRS = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"RandomizedSearchKNN\", clfKNNRS)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero entrenamos, validamos y obtenemos el accuracy en los datos del dataset Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68831168831168832"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsKNNPimaRS = estimatorKNNRS.fit(train_attsPima, train_labelPima)\n",
    "predictionKNNPimaRS = estimatorKNNRS.predict(test_attsPima)\n",
    "print('Accuracy Pima KNN with RS:')\n",
    "metrics.accuracy_score(test_labelPima, predictionKNNPimaRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos cuál ha sido la mejor configuración de parámetros obtenida por el RandomizedSearch. En este caso es utilizar 5 vecinos, usando la métrica uniforme para medir las distancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima KNN) con RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 5, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima KNN) con RS:')\n",
    "clsKNNPimaRS.named_steps['RandomizedSearchKNN'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación mostramos la matriz de confusión, precision y recall obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Pima KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[82, 18],\n",
       "       [30, 24]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Pima KNN with RS:')\n",
    "metrics.confusion_matrix(test_labelPima, predictionKNNPimaRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Pima KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44444444444444442"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Pima KNN with RS:')\n",
    "metrics.recall_score(test_labelPima, predictionKNNPimaRS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Pima KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Pima KNN with RS:')\n",
    "metrics.precision_score(test_labelPima, predictionKNNPimaRS, pos_label=\"tested_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos del dataset Wisconsin, entrenamos nuestro clasificador, validamos y obtenemos el accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsKNNWiscRS = estimatorKNNRS.fit(train_attsWisc, train_labelWisc)\n",
    "predictionKNNWiscRS = estimatorKNNRS.predict(test_attsWisc)\n",
    "print('Accuracy Wisconsin KNN with RS:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionKNNWiscRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mejores parámetros obtenidos por el RandomizedSearch para el KNN en este dataset son utilizar 1 solo vecino, usando la métrica uniforme para medir las distancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin KNN) con RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin KNN) con RS:')\n",
    "clsKNNWiscRS.named_steps['RandomizedSearchKNN'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos la matriz de confusión, precision y recall obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix Wisconsin KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[91,  1],\n",
       "       [ 6, 42]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix Wisconsin KNN with RS:')\n",
    "metrics.confusion_matrix(test_labelWisc, predictionKNNWiscRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Wisconsin KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "print('Recall Wisconsin KNN with RS:')\n",
    "metrics.recall_score(test_labelWisc, predictionKNNWiscRS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Wisconsin KNN with RS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97674418604651159"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "print('Precision Wisconsin KNN with RS:')\n",
    "metrics.precision_score(test_labelWisc, predictionKNNWiscRS, pos_label=\"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, los resultados usando RandomizedSearch y GridSearch son muy similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Naive Bayes con GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a usar el GridSearchCV con el clasificador Naive Bayes, concretamente el algoritmo GaussianNB, el cual se utiliza para clasificar con variables continuas. Este algoritmo solo recibe como parámetro 'priors' las probabilidades a priori de la clase. En nuestro caso, crearemos un GridSearch con el parámetro 'priors' fijado a diferentes valores.\n",
    "\n",
    "Una vez realizado el GridSearch, se lo pasaremos a un pipeline que tendrá dos pasos: un imputer para los valores perdidos, y el GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfNB = GridSearchCV(\n",
    "    estimator = GaussianNB(),\n",
    "    param_grid = \n",
    "        { 'priors':[[0.3,0.7],[0.5,0.5],[0.9,0.1],[0.6,0.4]] },\n",
    "    scoring = 'accuracy',\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed), \n",
    "    iid=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the pipeline as a set of tuples\n",
    "estimatorNB = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"GridSearchNB\", clfNB)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero entrenamos nuestro clasificador con el dataset Pima, y observamos que el accuracy es ligeramente mayor que con los otros clasificadores que probamos previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Pima NB:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.74025974025974028"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsNBPima = estimatorNB.fit(train_attsPima, train_labelPima)\n",
    "predictionNBPima = estimatorNB.predict(test_attsPima)\n",
    "print('Accuracy Pima NB:')\n",
    "metrics.accuracy_score(test_labelPima, predictionNBPima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos la mejor configuración de parámetros obtenida por el GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Pima NB):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'priors': [0.6, 0.4]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Pima NB):')\n",
    "clsNBPima.named_steps['GridSearchNB'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probamos con el dataset Wisconsin. El accuracy obtenido también es ligeramente mayor al de los clasificadores árbol y KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Wisconsin NB:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97857142857142854"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can fit and use the pipeline as usual\n",
    "clsNBWisc = estimatorNB.fit(train_attsWisc, train_labelWisc)\n",
    "predictionNBWisc = estimatorNB.predict(test_attsWisc)\n",
    "print('Accuracy Wisconsin NB:')\n",
    "metrics.accuracy_score(test_labelWisc, predictionNBWisc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos la mejor configuración de parámetros obtenida por el GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración de parámetros (Wisconsin NB):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'priors': [0.3, 0.7]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Mejor configuración de parámetros (Wisconsin NB):')\n",
    "clsNBWisc.named_steps['GridSearchNB'].best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
